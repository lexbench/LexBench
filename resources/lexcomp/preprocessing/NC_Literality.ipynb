{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Compounds \n",
    "### Literality\n",
    "\n",
    "We use the dataset from [Reddy et al. (2011)](http://www.aclweb.org/anthology/I11-1024). The dataset contains human judgements of the literality of 90 noun compounds, at both the compound and the constituent level. For example, in _sacred cow_, _sacred_ is literal while _cow_ is not. If a constituent is literal, then a sentence containing the noun compound is also affected by its meaning. For example, in a sentence discussing _olive oil_ the sentence is affected by the meanings of _olive_ and _oil_. Contrarily, a sentence containing _sacred cow_ is not affected by the meaning of _cow_. The scores in the dataset are in a scale of 0-5, 0 being non-literal and 5 being literal. We consider scores of at least 4 as literal, and at most 2 as non-literal, and ignore the 2-4 range. For each such noun compound, we look at all of its occurrences in Wikipedia, and sample some short sentences (up to 20 words). We create examples such as: `[sentence] [w1] [literal/non-literal]` and `[sentence] [w2] [literal/non-literal]`. We then make sure that there is no word `w1` and `w2` that constantly appears only in negative or only in positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(133)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import spacy\n",
    "import shutil\n",
    "import codecs\n",
    "import random\n",
    "import fileinput\n",
    "\n",
    "from nltk import agreement\n",
    "from itertools import count\n",
    "from collections import Counter, defaultdict\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('nc_literality'):\n",
    "    !mkdir -p nc_literality\n",
    "    !wget http://sivareddy.in/papers/files/ijcnlp_compositionality_data.tgz \n",
    "    !tar -zxvf ijcnlp_compositionality_data.tgz \n",
    "    !mv ijcnlp_compositionality_data/MeanAndDeviations.clean.txt nc_literality\n",
    "    !rm -r ijcnlp_compositionality_data\n",
    "    !rm -r ijcnlp_compositionality_data.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LITERAL = 4\n",
    "MAX_NON_LITERAL = 2\n",
    "\n",
    "with codecs.open('nc_literality/MeanAndDeviations.clean.txt', 'r', 'utf-8') as f_in:\n",
    "    lines = [line.strip().split('\\t') for line in f_in]\n",
    "    \n",
    "out_of_context_examples = []\n",
    "\n",
    "# Skip the header line\n",
    "for nc, data in lines[1:]:\n",
    "    data = data.split()\n",
    "    w1_mean, w2_mean = float(data[0]), float(data[2])\n",
    "    \n",
    "    # Remove the POS\n",
    "    w1, w2 = nc.split()\n",
    "    w1 = w1[:-2]\n",
    "    w2 = w2[:-2]\n",
    "    nc = '_'.join((w1, w2))\n",
    "    \n",
    "    for constituent, score in zip([w1, w2], [w1_mean, w2_mean]):\n",
    "        if score >= MIN_LITERAL:\n",
    "            out_of_context_examples.append((nc, constituent, 'LITERAL'))\n",
    "        elif score <= MAX_NON_LITERAL:\n",
    "            out_of_context_examples.append((nc, constituent, 'NON-LITERAL'))\n",
    "        \n",
    "ncs = set([nc for nc, constituent, label in out_of_context_examples])\n",
    "print(f'Number of examples from Reddy et al.: {len(out_of_context_examples)}')\n",
    "non_literal = len([label for _, _, label in out_of_context_examples if label == 'NON-LITERAL'])\n",
    "print(f'Number of non-literal examples: {non_literal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the size of the dataset, we will use additional noun compounds from the Tratz (2011) dataset. \n",
    "We can only trust that in *compositional* compounds, both constituents are literal (while in *non-compositional* compounds, one of the constituents may still be literal). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('tratz.tsv'):\n",
    "    !wget https://vered1986.github.io/papers/Tratz2011_Dataset.tar.gz\n",
    "    !tar -zxvf Tratz2011_Dataset.tar.gz\n",
    "    !cat Data/tratz2011_fine_grained_random/*.tsv > nc_literality/tratz.tsv\n",
    "    !rm -r Data\n",
    "    !rm -r Tratz2011_Dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1s, w2s = zip(*[nc.split('_') for nc in ncs])\n",
    "reddy_vocab = set(w1s).union(set(w2s))\n",
    "\n",
    "with codecs.open('nc_literality/tratz.tsv', 'r', 'utf-8') as f_in:\n",
    "    tratz_dataset = [line.strip().split('\\t') for line in f_in]\n",
    "    \n",
    "# Remove expressions with more than two words\n",
    "tratz_dataset = [(w1, w2, label) \n",
    "                 for w1, w2, label in tratz_dataset \n",
    "                 if ' ' not in w1 and ' ' not in w2]\n",
    "\n",
    "labels_to_remove = {'LEXICALIZED', 'PERSONAL_NAME', 'PERSONAL_TITLE'}\n",
    "\n",
    "tratz_out_of_context_examples = [('_'.join((w1, w2)), w1, 'LITERAL') \n",
    "                                 for w1, w2, label in tratz_dataset\n",
    "                                 if w1 in reddy_vocab and labels_to_remove and\n",
    "                                 ('_'.join((w1, w2)), w1, 'NON-LITERAL') not in out_of_context_examples]\n",
    "\n",
    "tratz_out_of_context_examples += [('_'.join((w1, w2)), w2, 'LITERAL') \n",
    "                                 for w1, w2, label in tratz_dataset\n",
    "                                 if w2 in reddy_vocab and labels_to_remove and\n",
    "                                 ('_'.join((w1, w2)), w2, 'NON-LITERAL') not in out_of_context_examples]\n",
    "\n",
    "# Make sure each NC-target pair has only one label\n",
    "label_per_nc_target = defaultdict(set)\n",
    "[label_per_nc_target[(nc, target)].add(label) for nc, target, label in out_of_context_examples]\n",
    "assert([len(lst) == 1 for lst in label_per_nc_target.values()])\n",
    "\n",
    "out_of_context_examples += tratz_out_of_context_examples\n",
    "print('Number of out of context examples: {}'.format(len(out_of_context_examples)))\n",
    "ncs = set([nc for nc, constituent, label in out_of_context_examples])\n",
    "print('Number of noun compounds: {}'.format(len(ncs)))\n",
    "literal = [label for _, _, label in out_of_context_examples if label == 'LITERAL']\n",
    "non_literal = [label for _, _, label in out_of_context_examples if label == 'NON-LITERAL']\n",
    "print(f'Literal: {len(literal)}, non-literal: {len(non_literal)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract all the corpus sentences in which the noun compound appears. We will generate a bash script that runs 60 `grep` commands in parallel. Change it as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_PARALLEL = 60\n",
    "\n",
    "corpus = '~/corpora/text/en_corpus_tokenized' # change to your corpus path\n",
    "out_dir = 'nc_literality/sentences'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    \n",
    "with codecs.open(os.path.join('nc_literality', 'commands.sh'), 'w', 'utf-8') as f_out:\n",
    "    f_out.write('#!/bin/bash\\n')\n",
    "    for i, nc in enumerate(ncs):\n",
    "        f_out.write('grep -i \"{}\" {} > sentences/{} &\\n'.format(nc.replace('_', ' '), corpus, nc))\n",
    "        if i > 0 and i % NUM_PARALLEL == 0:\n",
    "            f_out.write('wait\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out sentences which are too long or too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SENT_LEN = 15\n",
    "MAX_SENT_LEN = 25\n",
    "\n",
    "nc_sentences_filtered = {}\n",
    "for nc in ncs:\n",
    "    try:\n",
    "        with codecs.open(os.path.join(out_dir, nc), 'r', 'utf-8') as f_in:\n",
    "            nc_sentences = [line.strip() for line in f_in]\n",
    "\n",
    "        nc_sentences_filtered[nc] = [s for s in nc_sentences \n",
    "                                     if len(s.split()) <= MAX_SENT_LEN and \n",
    "                                     len(s.split()) >= MIN_SENT_LEN]\n",
    "\n",
    "        with codecs.open(os.path.join(out_dir, nc), 'w', 'utf-8') as f_out:\n",
    "            for s in nc_sentences_filtered[nc]:\n",
    "                f_out.write(s + '\\n')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "print('Number of noun compounds: {}'.format(len(nc_sentences_filtered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample up to 10 sentences for each out-of-context example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for nc, constituent, label in out_of_context_examples:\n",
    "    w1, w2 = nc.split('_')\n",
    "    curr_sentences = nc_sentences_filtered.get(nc, [])\n",
    "    sent_tokens = [sentence.split() for sentence in curr_sentences]\n",
    "    valid_sentences = [(sent, tokens) for sent, tokens in zip(curr_sentences, sent_tokens) \n",
    "                       if len([i for i, t in enumerate(tokens) \n",
    "                               if t == w1 and len(tokens) > i+1 and tokens[i+1] == w2]) > 0]\n",
    "    \n",
    "    if len(valid_sentences) > 0:\n",
    "        for sentence, tokens in random.sample(valid_sentences, min(10, len(valid_sentences))):\n",
    "\n",
    "            # Find the noun compound\n",
    "            nc_indices = [i for i, t in enumerate(tokens) \n",
    "                          if t == w1 and len(tokens) > i+1 and tokens[i+1] == w2]\n",
    "\n",
    "            # Find the target index\n",
    "            if len(nc_indices) > 0:\n",
    "                target_index = nc_indices[0] if constituent == w1 else nc_indices[0]+1\n",
    "                dataset.append((sentence, nc, target_index, label))\n",
    "                sentences_added += 1\n",
    "\n",
    "print(f'Number of examples: {len(dataset)}')\n",
    "ctr = Counter([label for (sentence, nc, target_index, label) in dataset])\n",
    "print(f'Literal: {ctr[\"LITERAL\"]}, Non-literal: {ctr[\"NON-LITERAL\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the dataset is not biased with respect to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ctr['NON-LITERAL'] / ctr['LITERAL'] < 0.25: \n",
    "    literal = [(sentence, nc, target_index, label) \n",
    "               for (sentence, nc, target_index, label) in dataset\n",
    "               if label == 'LITERAL']\n",
    "    non_literal = [(sentence, nc, target_index, label) \n",
    "                   for (sentence, nc, target_index, label) in dataset\n",
    "                   if label == 'NON-LITERAL']\n",
    "    dataset = non_literal + random.sample(literal, len(non_literal) * 4)\n",
    "\n",
    "print('Dataset size: {}'.format(len(dataset)))\n",
    "ctr = Counter([label for (sentence, nc, target_index, label) in dataset])\n",
    "print(f'Literal: {ctr[\"LITERAL\"]}, Non-literal: {ctr[\"NON-LITERAL\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset lexically by modifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lexically(dataset, word_index=0):\n",
    "    \"\"\"\n",
    "    Split the dataset to train, test, and validation, such that\n",
    "    the word in word_index (0 = modifier, 1 = head) doesn't\n",
    "    repeat across sets.\n",
    "    \"\"\"\n",
    "    literal_instances_per_w = defaultdict(list)\n",
    "    [literal_instances_per_w[nc.split('_')[word_index]].append(\n",
    "        (sentence, nc, target_index, label)) \n",
    "     for (sentence, nc, target_index, label) in dataset\n",
    "     if label == 'LITERAL']\n",
    "    \n",
    "    non_literal_instances_per_w = defaultdict(list)\n",
    "    [non_literal_instances_per_w[nc.split('_')[word_index]].append(\n",
    "        (sentence, nc, target_index, label)) \n",
    "     for (sentence, nc, target_index, label) in dataset\n",
    "     if label == 'NON-LITERAL']\n",
    "    \n",
    "    # First, split the non literal examples\n",
    "    words_in_non_literal = list(non_literal_instances_per_w.keys())\n",
    "    train_size = 8 * len(words_in_non_literal) // 10\n",
    "    val_size = test_size = len(words_in_non_literal) // 10\n",
    "    words_for_train = words_in_non_literal[:train_size]\n",
    "    words_for_val = words_in_non_literal[train_size+1:train_size+val_size]\n",
    "    words_for_test = words_in_non_literal[train_size+val_size+1:]\n",
    "\n",
    "    train = [ex for w in words_for_train for ex in non_literal_instances_per_w[w]]\n",
    "    val = [ex for w in words_for_val for ex in non_literal_instances_per_w[w]]\n",
    "    test = [ex for w in words_for_test for ex in non_literal_instances_per_w[w]]\n",
    "    \n",
    "    # Then add the literal ones\n",
    "    train += [ex for w in words_for_train for ex in literal_instances_per_w.get(w, [])]\n",
    "    val += [ex for w in words_for_val for ex in literal_instances_per_w.get(w, [])]\n",
    "    test += [ex for w in words_for_test for ex in literal_instances_per_w.get(w, [])]\n",
    "    \n",
    "    literal_instances_per_w = {w: examples \n",
    "                               for w, examples in literal_instances_per_w.items() \n",
    "                               if w not in set(words_in_non_literal)}\n",
    "    \n",
    "    train_size = 8 * len(dataset) // 10\n",
    "    val_size = test_size = len(dataset) // 10\n",
    "    \n",
    "    words = [w for w, examples in sorted(literal_instances_per_w.items(), key=lambda x: len(x[1]))]\n",
    "    w_index = 0\n",
    "\n",
    "    while len(test) < test_size:\n",
    "        test += literal_instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Test set size: {} (needed: {})'.format(len(test), test_size))\n",
    "\n",
    "    while len(val) < val_size:\n",
    "        val += literal_instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Validation set size: {} (needed: {})'.format(len(val), val_size))\n",
    "\n",
    "    train += [example for i in range(w_index, len(words)) \n",
    "              for example in literal_instances_per_w[words[i]]]\n",
    "    print('Train set size: {} (needed: {})'.format(len(train), train_size))\n",
    "\n",
    "    # Check the label distribution in the test set\n",
    "    ctr = Counter([label for (sentence, nc, target_index, label) in test])\n",
    "    if ctr['NON-LITERAL'] / ctr['LITERAL'] < 0.25: \n",
    "        literal = [(sentence, nc, target_index, label) \n",
    "                   for (sentence, nc, target_index, label) in test\n",
    "                   if label == 'LITERAL']\n",
    "        non_literal = [(sentence, nc, target_index, label) \n",
    "                       for (sentence, nc, target_index, label) in test\n",
    "                       if label == 'NON-LITERAL']\n",
    "        test = non_literal + random.sample(literal, len(non_literal) * 2)\n",
    "    \n",
    "    ctr = Counter([label for (sentence, nc, target_index, label) in test])\n",
    "    print(f'Test ratio: {ctr[\"NON-LITERAL\"] / ctr[\"LITERAL\"]}')\n",
    "    assert(0.25 <= ctr['NON-LITERAL'] / ctr['LITERAL'] <= 4)\n",
    "    \n",
    "    ctr = Counter([label for (sentence, nc, target_index, label) in train])\n",
    "    print(f'Train ratio: {ctr[\"NON-LITERAL\"] / ctr[\"LITERAL\"]}')\n",
    "    ctr = Counter([label for (sentence, nc, target_index, label) in val])\n",
    "    print(f'Val ratio: {ctr[\"NON-LITERAL\"] / ctr[\"LITERAL\"]}')\n",
    "    \n",
    "    # Make sure the split is lexical among verbs\n",
    "    test_words = [nc.split('_')[word_index] for sentence, nc, target_index, label in test]\n",
    "    train_words = [nc.split('_')[word_index] for sentence, nc, target_index, label in train]\n",
    "    val_words = [nc.split('_')[word_index] for sentence, nc, target_index, label in val]\n",
    "    assert(len(set(train_words).intersection(set(val_words))) == 0)\n",
    "    assert(len(set(train_words).intersection(set(test_words))) == 0)\n",
    "    assert(len(set(test_words).intersection(set(val_words))) == 0)\n",
    "\n",
    "    print(f'Sizes: train = {len(train)}, test = {len(test)}, validation = {len(val)}')\n",
    "    return train, test, val\n",
    "    \n",
    "\n",
    "data_dir = '../diagnostic_classifiers/data/nc_literality'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "train, test, val = split_lexically(dataset, word_index=1)\n",
    "\n",
    "# train_size = 8 * len(dataset) // 10\n",
    "# val_size = test_size = len(dataset) // 10\n",
    "# train = dataset[:train_size]\n",
    "# val = dataset[train_size+1:train_size+val_size]\n",
    "# test = dataset[train_size+val_size+1:]\n",
    "# print(f'Sizes: train = {len(train)}, test = {len(test)}, validation = {len(val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the majority baseline by head is not too strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_majority_baseline(word_index=1):\n",
    "    train_labels = [label for sentence, nc, target_index, label in train]\n",
    "    test_labels = [label for sentence, nc, target_index, label in test]\n",
    "\n",
    "    per_word_labels = defaultdict(list)\n",
    "\n",
    "    for sentence, nc, target_index, label in train:\n",
    "        per_word_labels[nc.split('_')[word_index]].append(label)\n",
    "\n",
    "    curr_majority_labels = {w: Counter(labels).most_common(1)[0][0] \n",
    "                            for w, labels in per_word_labels.items()}\n",
    "    overall_majority = Counter(test_labels).most_common(1)[0][0]\n",
    "    print(f'Overall majority: {overall_majority}')\n",
    "\n",
    "    # Predict. If the word is not there, take the overall majority label.\n",
    "    test_predictions = []\n",
    "    for sentence, nc, target_index, label in test:\n",
    "        test_predictions.append(curr_majority_labels.get(nc.split('_')[word_index], overall_majority))\n",
    "\n",
    "    # Evaluate\n",
    "    acc = accuracy_score(test_labels, test_predictions)\n",
    "    word = ['modifier', 'head'][word_index]\n",
    "    print('Majority by {}: {:2.1f}%'.format(word, acc * 100.0))\n",
    "    \n",
    "    \n",
    "compute_majority_baseline(0)\n",
    "compute_majority_baseline(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s, filename in zip([train, test, val], ['train', 'test', 'val']):\n",
    "    with codecs.open(os.path.join(data_dir, '{}.jsonl'.format(filename)), 'w', 'utf-8') as f_out:\n",
    "        for sentence, nc, target_index, label in s:\n",
    "            example_dict = {'sentence' : sentence, 'nc': nc, 'target_index': target_index, \n",
    "                            'target_word': sentence.split()[target_index], 'label': label}\n",
    "            f_out.write(json.dumps(example_dict) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-annotated a sample from the test set to compute human performance. \n",
    "We assume the annotation results are found under `preprocessing/annotation/nc_literality/batch_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_results(result_file, remove_bad_workers=False):\n",
    "    \"\"\"\n",
    "    Load the batch results from the CSV\n",
    "    :param result_file: the batch results CSV file from MTurk\n",
    "    :return: the workers and the answers\n",
    "    \"\"\"\n",
    "    answer_by_worker, answer_by_hit = defaultdict(dict), defaultdict(dict)\n",
    "    workers = set()\n",
    "    incorrect = set()\n",
    "    workers_wrong_answers = defaultdict(int)\n",
    "    hit_id_to_orig_label = {}\n",
    "    \n",
    "    with codecs.open(result_file, 'r', 'utf-8') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in reader:\n",
    "            hit_id = row['HITId']\n",
    "            worker_id = row['WorkerId']\n",
    "\n",
    "            # Input fields\n",
    "            sent = row['Input.sent']\n",
    "            orig_label = row['Input.orig_label']\n",
    "            \n",
    "            tokens = sent.split()\n",
    "            \n",
    "            try:\n",
    "                w1 = [t for t in tokens if t.startswith('<mark>')][0].replace('<mark>', '')\n",
    "                w2 = [t for t in tokens if t.endswith('</mark>')][0].replace('</mark>', '')\n",
    "                sent = sent.replace('<mark>', '').replace('</mark>', '').strip()\n",
    "            except:\n",
    "                print(f'Warning: skipped \"{sentence}\"')\n",
    "                continue\n",
    "            \n",
    "            hit_id_to_orig_label[hit_id] = orig_label\n",
    "            \n",
    "            # Answer fields\n",
    "            if row['Answer.label.literal'].lower() == 'true':\n",
    "                answer = 'LITERAL'\n",
    "            elif row['Answer.label.non_literal'].lower() == 'true':\n",
    "                answer = 'NON-LITERAL'\n",
    "            # Incorrect\n",
    "            else:\n",
    "                incorrect.add(hit_id)\n",
    "                continue\n",
    "                \n",
    "            if orig_label != answer:\n",
    "                workers_wrong_answers[worker_id] += 1\n",
    "                \n",
    "            workers.add(worker_id)\n",
    "            answer_by_worker[worker_id][hit_id] = answer\n",
    "            answer_by_hit[hit_id][worker_id] = answer\n",
    "            \n",
    "    # Remove HITs that were annotated as incorrect by at least one worker\n",
    "    answer_by_hit = {hit_id: answers_by_hit_id \n",
    "                     for hit_id, answers_by_hit_id in answer_by_hit.items()\n",
    "                     if hit_id not in incorrect}\n",
    "    \n",
    "    new_answer_by_worker = {}\n",
    "    for worker_id, curr_answers in answer_by_worker.items():\n",
    "        new_answer_by_worker[worker_id] = {hit_id: answer \n",
    "                                           for hit_id, answer in curr_answers.items()\n",
    "                                           if hit_id not in incorrect}\n",
    "        \n",
    "    answer_by_worker = new_answer_by_worker\n",
    "    num_answers = sum([len(answers_by_worker_id) \n",
    "                       for answers_by_worker_id in answer_by_worker.values()])\n",
    "    \n",
    "    if remove_bad_workers:\n",
    "        workers_wrong_answers = {worker_id: n * 100.0 / len(answer_by_worker[worker_id])\n",
    "                                 for worker_id, n in workers_wrong_answers.items()}\n",
    "\n",
    "        # Remove bad workers: workers that disagreed with many of the previous annotation \n",
    "        bad_workers = {worker_id \n",
    "                       for worker_id, per in workers_wrong_answers.items() if per > 33}\n",
    "        print(f'Removing {len(bad_workers)} bad workers:\\n{bad_workers}')\n",
    "\n",
    "        answer_by_worker = {worker_id: answers_by_worker_id \n",
    "                            for worker_id, answers_by_worker_id in answer_by_worker.items()\n",
    "                            if worker_id not in bad_workers}\n",
    "\n",
    "        for hit_id in answer_by_hit.keys():\n",
    "            answers_by_hit_id = answer_by_hit[hit_id]\n",
    "            answer_by_hit[hit_id] = {worker_id: answer \n",
    "                                      for worker_id, answer in answers_by_hit_id.items()\n",
    "                                      if worker_id not in bad_workers}\n",
    "\n",
    "        num_answers_after_filtering = sum([len(answers_by_worker_id) \n",
    "                                           for answers_by_worker_id in answer_by_worker.values()])\n",
    "        print('Final: {} answers, removed {}.'.format(\n",
    "            num_answers_after_filtering, \n",
    "            num_answers - num_answers_after_filtering))\n",
    "    \n",
    "    return workers, answer_by_worker, answer_by_hit, incorrect, hit_id_to_orig_label\n",
    "\n",
    "\n",
    "results_file = 'nc_literality/batch_results.csv'\n",
    "workers, answer_by_worker, answer_by_hit, incorrect, hit_id_to_orig_label = load_batch_results(\n",
    "    results_file, remove_bad_workers=True)\n",
    "print(f'Loaded results from {results_file}, loaded {len(answer_by_hit)} HITs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes Fleiss Kappa and percent of agreement between the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement(answer_by_hit):\n",
    "    \"\"\"\n",
    "    Compute workers' agreement (Fleiss Kappa and percent) \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    percent = 0\n",
    "    \n",
    "    for hit_id, worker_answers in answer_by_hit.items():\n",
    "        curr = [0, 0]\n",
    "\n",
    "        for answer in worker_answers.values():\n",
    "            label = 1 if answer == 'LITERAL' else 0\n",
    "            curr[label] += 1\n",
    "            \n",
    "        if sum(curr) == 3:\n",
    "            data.append(curr)\n",
    "            curr_agreement = sum([max(0, a-1) for a in curr])        \n",
    "            percent += curr_agreement\n",
    "\n",
    "    kappa = fleiss_kappa(data)\n",
    "    percent = percent * 100.0 / (len(data) * 2)\n",
    "    return kappa, percent\n",
    "\n",
    "\n",
    "kappa, percent = compute_agreement(answer_by_hit)\n",
    "print('Fleiss Kappa={:.3f}, Percent={:.2f}%'.format(kappa, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the workers majority which we will use to estimate human performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_majority(results):\n",
    "    \"\"\"\n",
    "    Compute the majority label from the worker answers    \n",
    "    :param results: HIT ID to worker answers dictionary\n",
    "    \"\"\"\n",
    "    distribution = { hit_id : Counter(sent_results.values())\n",
    "                    for hit_id, sent_results in results.items() }\n",
    "    \n",
    "    dataset = {}\n",
    "    for hit_id, dist in distribution.items():\n",
    "        if len(dist) > 0 and dist.most_common(1)[0][1] >= 2:\n",
    "            label = dist.most_common(1)[0][0]\n",
    "            dataset[hit_id] = label\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "human_annotations = compute_majority(answer_by_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the human performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_compared = [1 if human_annotations[hit_id].lower() == hit_id_to_orig_label[hit_id].lower() else 0 \n",
    "                  for hit_id in human_annotations.keys()]\n",
    "            \n",
    "human_accuracy = sum(items_compared) * 100.0 / len(items_compared)\n",
    "\n",
    "print('Number of examples: {}, accuracy: {:.3f}'.format(len(items_compared), human_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexcomp)",
   "language": "python",
   "name": "lexcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
