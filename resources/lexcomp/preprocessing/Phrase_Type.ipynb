{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Type\n",
    "### Identify the type of multiword expressions\n",
    "\n",
    "The task is to identify spans of phrases and their types, and it is designed as a sequence labeling (BIO) task. We use the STREUSLE corpus [(Schneider and Smith, 2015)](http://aclweb.org/anthology/N15-1177): Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions. The text is from the web reviews portion of the English Web Treebank. We are going to use the MWE annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(133)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import spacy\n",
    "import shutil\n",
    "import codecs\n",
    "import random\n",
    "import fileinput\n",
    "\n",
    "from nltk import agreement\n",
    "from itertools import count\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the STREUSLE corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('mwe_type'):\n",
    "    !mkdir mwe_type\n",
    "    !wget https://github.com/nert-nlp/streusle/raw/master/streusle.conllulex -O mwe_type/streusle.conllulex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the discontinuous spans and label the \"weak\" (compositional) MWEs as \"COMP\". Thanks to Nathan Schneider for the following code, that we adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Converts tagged sentences into a simpler form without gaps and/or weak links. \n",
    "If the mode is gaps+weak, 4 versions of each sentence are produced. \n",
    "Otherwise, 2 versions are produced, corresponding to liberal and conservative \n",
    "conversion rules.\n",
    "\n",
    "Args: gaps|weak|gaps+weak TAGGED_FILE\n",
    "\n",
    "@author: Nathan Schneider (nschneid@cs.cmu.edu)\n",
    "@since: 2013-06-30\n",
    "'''\n",
    "def is_tag(t):\n",
    "    return t in {'B','b','O','o','I','I_','I~','i','i~','i~'}\n",
    "\n",
    "RE_TAGGING = re.compile(r'^(O|B(o|b[ii~i~]+|[II_I~])*[II_I~]+)+$')\n",
    "\n",
    "def require_valid_tagging(tagging, simplify_gaps, simplify_weak):\n",
    "    assert re.match(r'^(O|B(o|b[ii~i~]+|[II_I~])*[II_I~]+)+$', tagging)\n",
    "    if simplify_gaps:\n",
    "        assert re.match(r'^(O|B[II_I~]+)+$', tagging)\n",
    "    if simplify_weak:\n",
    "        assert re.match(r'^(O|B(o|bi+|I)*I+)+$', tagging)\n",
    "\n",
    "IN_GAP_TAGS = {'o','b','i','i~','i~'}\n",
    "I_TILDE = 'I~'\n",
    "i_TILDE = 'i~'\n",
    "I_BAR = 'I_'\n",
    "i_BAR = 'i~'\n",
    "\n",
    "def simplify(sentid, tokens, poses, tags, simplification='gaps+weak', policy='best'):\n",
    "    '''\n",
    "    For each possible conversion of the sentence under the given simplification scheme, \n",
    "    modifies the gold tags and yields the instance weight for the simplified version \n",
    "    (such that all weights are equal and sum to 1). Then restores the original \n",
    "    gold tags.\n",
    "    '''\n",
    "    assert simplification in {'gaps', 'weak', 'gaps+weak'}\n",
    "    assert policy in {'all', 'best'}\n",
    "    BEST_POLICY_RESULT = {'weak': 1, # the high-recall (liberal) policy: convert weak to strong\n",
    "                          'gaps': 0, # the high-precision (conservative) policy: remove cross-gap links\n",
    "                          'gaps+weak': 1}   # combination of the above\n",
    "    \n",
    "    gold_tags = set(tags)\n",
    "    assert gold_tags<=set('OoBbIi') | {'I_','I~','i~','i~'}\n",
    "    simplify_gaps = simplification in {'gaps','gaps+weak'} and not gold_tags<={'O','B','I','I_','I~'}\n",
    "    simplify_weak = simplification in {'weak','gaps+weak'} and not gold_tags<=set('OoBbIi')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if simplify_gaps or simplify_weak:\n",
    "        tt = list(tags) # output tags\n",
    "        if simplify_gaps:\n",
    "            assert 'o' in gold_tags or 'b' in gold_tags\n",
    "            # conservative: remove gaps from gappy expressions\n",
    "            for i,orig in enumerate(tags):\n",
    "                if tt[i] in IN_GAP_TAGS:\n",
    "                    if tt[i-1] not in IN_GAP_TAGS:\n",
    "                        if tt[i-1]=='B':\n",
    "                            tt[i-1] = 'O'\n",
    "                    #tt[i] = 'O'\n",
    "                    if tt[i+1] not in IN_GAP_TAGS:\n",
    "                        # a strong or weak I tag\n",
    "                        tt[i+1] = 'B' if i+2<len(tags) else 'O'\n",
    "                elif tt[i] in {'O','B'} and i>0 and tt[i-1]=='B':\n",
    "                    # we introduced a B after a gap which is actually a singleton, so remove it\n",
    "                    tt[i-1] = 'O'\n",
    "                    \n",
    "            result = [t.upper().replace(i_TILDE, I_TILDE).replace(i_BAR, I_BAR) for t in tt]\n",
    "            require_valid_tagging(''.join(result), simplify_gaps, False)\n",
    "            results.append(result)\n",
    "            \n",
    "            tt = list(tags)\n",
    "            # liberal: link across gaps (weakly if possible, preserving in-gap strong MWEs)\n",
    "            for i,orig in enumerate(tags):\n",
    "                if tt[i] in IN_GAP_TAGS:\n",
    "                    if simplify_weak:\n",
    "                        tt[i] = 'I'\n",
    "                    elif tt[i]=='b':\n",
    "                        if tt[i-1]=='B':\n",
    "                            tt[i] = I_TILDE\n",
    "                    else:\n",
    "                        if tt[i]==i_TILDE and tags[i-1]=='b':\n",
    "                            # weak link within a gap: merge it with the cross-gap weak MWE\n",
    "                            tt[i-1] = I_TILDE\n",
    "                        if tt[i]!=i_BAR:\n",
    "                            tt[i] = I_TILDE\n",
    "                elif not simplify_weak and tt[i]==I_BAR and tags[i-1] in IN_GAP_TAGS:\n",
    "                    # post-gap continuation should be weak\n",
    "                    tt[i] = I_TILDE\n",
    "            \n",
    "            \n",
    "            result = [t.upper().replace(i_TILDE, I_TILDE).replace(i_BAR, I_BAR) for t in tt]\n",
    "            require_valid_tagging(''.join(result), simplify_gaps, False)\n",
    "            results.append(result)\n",
    "        else:\n",
    "            results.append(list(tags))\n",
    "            if 'gaps' in simplification:\n",
    "                results.append(list(tags))\n",
    "        \n",
    "        assert len(results)==(2 if 'gaps' in simplification else 1),results\n",
    "        \n",
    "        if simplify_weak:\n",
    "            partial_results = results\n",
    "            results = []\n",
    "            for partial_result in partial_results:\n",
    "                # conservative: remove weak links\n",
    "                tt = list(partial_result)\n",
    "                \n",
    "                # - convert weak I's to B's, and strong I's to plain I's\n",
    "                tt[:] = [{i_TILDE: 'b', I_TILDE: 'B', i_BAR: 'i', I_BAR: 'I'}.get(t,t) for t in tt]\n",
    "                # - remove trans-gap weak links\n",
    "                for i,t in enumerate(tt):\n",
    "                    if t=='B' and i>0 and tt[i-1] in {'o','i'}:\t# B after gap. the trans-gap link was weak, so everything inside the gap becomes no longer gappy\n",
    "                        j = i-1\n",
    "                        while tt[j].islower():\n",
    "                            tt[j] = tt[j].upper()\n",
    "                            j -= 1\n",
    "                        if tt[j]=='B':\n",
    "                            tt[j] = 'O'\n",
    "                # - remove singleton B's (B must be followed by I or a gap)\n",
    "                for i,t in enumerate(tt):\n",
    "                    if t=='B':\n",
    "                        if i+1==len(tt):    # B at end of sequence\n",
    "                            tt[i] = 'O'\n",
    "                        elif i>0 and tt[i-1]=='b':\n",
    "                            assert False\n",
    "                        elif tt[i+1] in {'O', 'B'}: # singleton B\n",
    "                            tt[i] = 'O'\n",
    "                    elif t=='b':\n",
    "                        if tt[i+1]!='i':\n",
    "                            tt[i] = 'o'\n",
    "                # TODO: weak trans-gap link\n",
    "                require_valid_tagging(''.join(tt), simplify_gaps, simplify_weak)\n",
    "                results.append(tt)\n",
    "                \n",
    "                # liberal: convert weak links to strong links\n",
    "                tt = list(partial_result)\n",
    "                for i,t in enumerate(tt):\n",
    "                    if t in {i_TILDE, i_BAR}:\n",
    "                        tt[i] = 'i'\n",
    "                    elif t in {I_TILDE, I_BAR}:\n",
    "                        tt[i] = 'I'\n",
    "\n",
    "                require_valid_tagging(''.join(tt), simplify_gaps, simplify_weak)\n",
    "                results.append(tt)\n",
    "        elif 'weak' in simplification:\n",
    "            results.append(list(results[0]))\n",
    "            results.append(list(results[1]))\n",
    "            \n",
    "        assert len(results)==(4 if simplification=='gaps+weak' else 2),(simplify_gaps,simplify_weak,results)\n",
    "\n",
    "    else:       # nothing to do for this sentence\n",
    "        for x in range(4 if simplification=='gaps+weak' else 2):\n",
    "            results.append(list(tags))\n",
    "    \n",
    "    if policy=='best':\n",
    "        results = [results[BEST_POLICY_RESULT[simplification]]]\n",
    "    \n",
    "    for result in results:\n",
    "        assert len(tokens)==len(result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify and convert to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "\n",
    "def add_sent_to_json(sent, f_out):\n",
    "    words, poses, prev_tags, labels = zip(*sent)\n",
    "    new_tags = simplify(sentid, words, poses, prev_tags)[0]\n",
    "    tags = ['-'.join((t, l))\n",
    "            if l != '' and t != 'O' else t \n",
    "            for t, l in zip(new_tags, labels)]\n",
    "    \n",
    "    # Remove empty, rare, and '!!@' (\"needs to be manually corrected\") labels \n",
    "    labels_to_remove = {'@', 'NUM', 'INF', 'INTJ'}\n",
    "    new_tags = []\n",
    "    i = 0\n",
    "    repl_next = False\n",
    "    \n",
    "    while i < len(tags):\n",
    "        # B with @ or with no label\n",
    "        if tags[i] == 'B' or (any([c in tags[i] for c in labels_to_remove]) and 'B' in tags[i]):\n",
    "            new_tags.append('O')\n",
    "            repl_next = True\n",
    "            \n",
    "        # I after B with @\n",
    "        elif repl_next and 'I' in tags[i]: \n",
    "            new_tags.append('O')\n",
    "            \n",
    "        # I with @ after regular B\n",
    "        elif '@' in tags[i] and 'I' in tags[i]:\n",
    "            new_tags.append('O')\n",
    "            for prev in range(len(new_tags) - 1, 0, -1):\n",
    "                orig_tag = new_tags[prev]\n",
    "                new_tags[prev] = 'O'\n",
    "                if orig_tag.startswith('B'):\n",
    "                    break \n",
    "        \n",
    "        # Regular tag\n",
    "        else:\n",
    "            new_tags.append(tags[i])\n",
    "            repl_next = False\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    f_out.write(json.dumps({'sentence_words': words, 'sentence_tags': new_tags}) + '\\n')\n",
    "    \n",
    "\n",
    "sentences_skipped = 0\n",
    "with codecs.open('mwe_type/streusle.jsonl', 'w', 'utf-8') as f_out:\n",
    "    with codecs.open('mwe_type/streusle.conllulex', 'r', 'utf-8') as f_in:\n",
    "        for line in f_in:  \n",
    "            if not line.strip():\n",
    "                if sent:\n",
    "                    try:\n",
    "                        add_sent_to_json(sent, f_out)\n",
    "                    except Exception as e:\n",
    "                        sentences_skipped += 1\n",
    "                        pass\n",
    "                    sent = []\n",
    "                continue\n",
    "\n",
    "            if line.startswith('#'):\n",
    "                if 'streusle_sent_id' in line:\n",
    "                    sentid = line.strip().split('=')[1][1:]\n",
    "                continue\n",
    "\n",
    "            data = line.strip().split('\\t')\n",
    "\n",
    "            if len(data) == 19:\n",
    "                _, tok, lemma, _, pos = data[:5]\n",
    "                pos = [lemma, pos]\n",
    "                tag = data[-1]\n",
    "\n",
    "                # Separate the specific label (MWE type) from the BIO tag\n",
    "                label = ''\n",
    "                if '-' in tag:\n",
    "                    tag, label = tag.split('-')[:2]\n",
    "                   \n",
    "                    # Weak MWE - we want the label to be just for the first word in the expression\n",
    "                    if 'I' in tag.upper() and label != '':\n",
    "                        for prev in range(len(sent) - 1, 0, -1):\n",
    "                            if 'B' in sent[prev][-2].upper():\n",
    "                                sent[prev][-1] = 'COMP'\n",
    "                                break\n",
    "                        label = ''\n",
    "                \n",
    "                # B\n",
    "                if not tag.upper().startswith('O') and not tag.upper().startswith('I'):\n",
    "                    # Phrasal verb\n",
    "                    if label.startswith('V.'):\n",
    "                        label = label.split('.')[1]\n",
    "                    # Regular label\n",
    "                    else:\n",
    "                        label = label.split('.')[0]\n",
    "                    \n",
    "                # O / I - no label\n",
    "                else:\n",
    "                    label = ''\n",
    "\n",
    "                sent.append([tok, pos, tag, label])\n",
    "\n",
    "    # Last sentence\n",
    "    if sent:\n",
    "        try:\n",
    "            add_sent_to_json(sent, f_out)\n",
    "        except Exception as e:\n",
    "            sentences_skipped += 1\n",
    "            pass\n",
    "        \n",
    "print(f'Sentences skipped: {sentences_skipped}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('mwe_type/streusle.jsonl', 'r', 'utf-8') as f_in:\n",
    "    dataset = [json.loads(line.strip()) for line in f_in]\n",
    "\n",
    "all_tags = [t for example in dataset for t in example['sentence_tags']]\n",
    "tagset = set(all_tags)\n",
    "print(f'Number of tags: {len(tagset)}')\n",
    "\n",
    "explanation = {'VPC': 'Verb-Particle Construction', \n",
    "               'LVC': 'Light Verb Construction',\n",
    "               'IAV': 'Inherently Adpositional Verb', \n",
    "               'VID': 'Verbal Idiom',\n",
    "               'N': 'Noun, common or proper',\n",
    "               'POSS.PRON': 'Possessive Pronoun',\n",
    "               'PRON': 'Non-possessive Pronoun', \n",
    "               'POSS': 'Possessive Clitic', \n",
    "               'V': 'Full verb or copula', \n",
    "               'AUX' : 'Auxiliary', \n",
    "               'P': 'Adposition',\n",
    "               'PP': 'Prepositional Phrase MWE', \n",
    "               'INF': 'Infinitive marker', \n",
    "               'DISC': 'Discourse / Pragmatic expression',\n",
    "               'ADJ': 'Adjective', \n",
    "               'ADV': 'Adverb', \n",
    "               'DET': 'Determiner', \n",
    "               'CCONJ': 'Conjunction', \n",
    "               'SCONJ': 'Subordinating Conjunction', \n",
    "               'INTJ': 'Interjection', \n",
    "               'NUM': 'Numeral', \n",
    "               'SYM': 'Symbol', \n",
    "               'PUNCT': 'Punctuation', \n",
    "               'X': 'Other',\n",
    "               'COMP': 'Weak Compositional MWE'\n",
    "              }\n",
    "\n",
    "print('\\n'.join((['\\t'.join((tag, explanation.get(tag.split('-')[-1], tag), str(count))) \n",
    "                  for tag, count in Counter(all_tags).most_common()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train, test, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset size: {}'.format(len(dataset)))\n",
    "\n",
    "train_size = 8 * len(dataset) // 10\n",
    "val_size = test_size = len(dataset) // 10\n",
    "random.shuffle(dataset)\n",
    "train = dataset[:train_size]\n",
    "test = dataset[train_size:train_size + test_size] \n",
    "val = dataset[train_size + test_size + 1:]\n",
    "\n",
    "# Remove examples that repeat across sets\n",
    "train_sents = set([' '.join(e['sentence_words']) for e in train])\n",
    "val_sents = set([' '.join(e['sentence_words']) for e in val])\n",
    "test_sents = set([' '.join(e['sentence_words']) for e in test])\n",
    "\n",
    "train = [e for e in train if ' '.join(e['sentence_words']) not in val_sents.union(test_sents)]\n",
    "val = [e for e in val if ' '.join(e['sentence_words']) not in train_sents.union(test_sents)]\n",
    "test = [e for e in test if ' '.join(e['sentence_words']) not in val_sents.union(train_sents)]\n",
    "\n",
    "train_sents = set([' '.join(e['sentence_words']) for e in train])\n",
    "val_sents = set([' '.join(e['sentence_words']) for e in val])\n",
    "test_sents = set([' '.join(e['sentence_words']) for e in test])\n",
    "\n",
    "print('Train set size: {}, test set size: {}, validation set size: {}'.format(len(train), len(test), len(val)))\n",
    "assert(len(train_sents.intersection(val_sents)) == 0)\n",
    "assert(len(train_sents.intersection(test_sents)) == 0)\n",
    "assert(len(test_sents.intersection(val_sents)) == 0)\n",
    "\n",
    "data_dir = '../diagnostic_classifiers/data/mwe_type/streusle'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "for s, filename in zip([train, test, val], ['train', 'test', 'val']):\n",
    "    with codecs.open(os.path.join(data_dir, '{}.jsonl'.format(filename)), 'w', 'utf-8') as f_out:\n",
    "        for e in s:\n",
    "            f_out.write(json.dumps(e) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexcomp)",
   "language": "python",
   "name": "lexcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
