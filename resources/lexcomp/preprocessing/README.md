## Preprocessing


### Downloading Wikipedia dump

We use Wikipedia to create some of the datasets. 
Specifically, we used the Wikipedia English dump from January 2018.
Download this dump and clean the text using [WikiExtractor](https://github.com/attardi/wikiextractor). 

```
bash corpus/download_corpus.sh [corpus_dir]
```

### Creating the datasets

Use the Jupyter notebooks in this directory to create each dataset.
