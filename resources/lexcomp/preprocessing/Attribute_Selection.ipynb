{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjective-Noun Compositions\n",
    "### Attribute Selection\n",
    "\n",
    "We use the HeiPLAS data set [(Hartung, 2015)](https://pub.uni-bielefeld.de/record/2900430), which contains adjective-attribute-noun triples that were heuristically extracted from WordNet and manually filtered by linguists. For example, \"_hot summer_\" (temperature) vs. \"_hot debate_\" (manner). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(133)\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import spacy\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter, defaultdict\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('an_classification/HeiPLAS-dev.txt'):\n",
    "    !mkdir -p an_classification\n",
    "    !wget http://www.cl.uni-heidelberg.de/~hartung/data/HeiPLAS-release.tgz\n",
    "    !tar -zvxf HeiPLAS-release.tgz\n",
    "    !mv HeiPLAS-release/HeiPLAS-* an_classification\n",
    "    !rm -r HeiPLAS-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = []\n",
    "\n",
    "for s in ['test', 'dev']:\n",
    "    with codecs.open('an_classification/HeiPLAS-{}.txt'.format(s), 'r', 'utf-8') as f_in:\n",
    "        original_dataset += [tuple(line.strip().lower().split()) for line in f_in]        \n",
    "\n",
    "print('Datset size: {}'.format(len(original_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the dataset as is (over 200 labels, which are similar words to the examples), \n",
    "we will create two examples for each _(AN, label)_ example in the dataset: one with _(AN, label, TRUE)_ and _(AN, another label, FALSE)_ for some other label which appears for either the adjective or the noun.\n",
    "\n",
    "What we need to do:\n",
    "\n",
    "* Create the new dataset.\n",
    "* Extract context sentences.\n",
    "* Filter out too long or too short sentences.\n",
    "* Sample a sentence for each example.\n",
    "* Split to train, test, and validation (ignoring the current split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_by_word = defaultdict(list)\n",
    "[label_by_word[a].append(label) for label, a, n in original_dataset]\n",
    "[label_by_word[n].append(label) for label, a, n in original_dataset]\n",
    "\n",
    "label_by_word = {w: Counter(labels) for w, labels in label_by_word.items()}\n",
    "\n",
    "new_dataset = []\n",
    "\n",
    "for label, a, n in original_dataset:\n",
    "    other_labels = sorted(list(label_by_word[a].items()) + list(label_by_word[n].items()), \n",
    "                          key=lambda x: x[1], reverse=True)\n",
    "    other_labels = list(set([l for l, c in other_labels if l != label]))\n",
    "    other_synsets = [wn.synsets(label) for label in other_labels]\n",
    "    \n",
    "    if len(other_labels) > 0:\n",
    "        # Pick a label which is not similar to the true label. \n",
    "        # The labels are originally from WordNet, so we will use WordNet similarity.\n",
    "        curr_syn = wn.synsets(label)\n",
    "        \n",
    "        if len(curr_syn) == 0: \n",
    "            continue\n",
    "            \n",
    "        curr_syn = curr_syn[0]\n",
    "        \n",
    "        other_labels = [l for l, syns in zip(other_labels, other_synsets)\n",
    "                        if len(syns) > 0\n",
    "                        if curr_syn.wup_similarity(syns[0]) < 0.4]\n",
    "        \n",
    "        if len(other_labels) > 0:\n",
    "            new_dataset += [(a, n, label, 'True')]\n",
    "            num_samples = min(3, len(other_labels))\n",
    "            for other_label in random.sample(other_labels, num_samples):\n",
    "                new_dataset += [(a, n, other_label, 'False')]\n",
    "\n",
    "print('Number of examples: {}'.format(len(new_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add context examples from Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'an_classification'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "ans = {'_'.join((w1, w2)) for w1, w2, relation, label in new_dataset}\n",
    "print('Number of adjective-noun compositions: {}'.format(len(ans)))\n",
    "\n",
    "corpus = '~/corpora/text/en_corpus_tokenized' # change to your corpus path\n",
    "out_dir = os.path.join(data_dir, 'sentences')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    \n",
    "NUM_PARALLEL = 60\n",
    "\n",
    "with codecs.open(os.path.join(data_dir, 'commands.sh'), 'w', 'utf-8') as f_out:\n",
    "    f_out.write('#!/bin/bash\\n')\n",
    "    commands_written = 0\n",
    "    for an in ans:\n",
    "        f_out.write('grep -i \"{}\" {} > sentences/{} &\\n'.format(an.replace('_', ' '), corpus, an))\n",
    "        commands_written += 1\n",
    "\n",
    "        if commands_written > 0 and commands_written % NUM_PARALLEL == 0:\n",
    "            f_out.write('wait\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_SENT_LEN = 15\n",
    "MAX_SENT_LEN = 25\n",
    "\n",
    "out_dir = os.path.expanduser('~/git/lexical_composition/preprocessing/an_classification/sentences')\n",
    "\n",
    "an_sentences_filtered = {}\n",
    "for an in ans:\n",
    "    try:\n",
    "        with codecs.open(os.path.join(out_dir, an), 'r', 'utf-8') as f_in:\n",
    "            an_sentences = [line.strip() for line in f_in]\n",
    "\n",
    "        an_sentences_filtered[an] = [s for s in an_sentences \n",
    "                                     if len(s.split()) <= MAX_SENT_LEN and \n",
    "                                     len(s.split()) >= MIN_SENT_LEN]\n",
    "\n",
    "        with codecs.open(os.path.join(out_dir, an), 'w', 'utf-8') as f_out:\n",
    "            for s in an_sentences_filtered[an]:\n",
    "                f_out.write(s + '\\n')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_dataset = []\n",
    "\n",
    "for w1, w2, relation, label in new_dataset:\n",
    "    an = '_'.join((w1, w2))\n",
    "    curr_sentences = []\n",
    "    \n",
    "    for sentence in an_sentences_filtered.get(an, []):\n",
    "        # The corpus was already tokenized\n",
    "        tokens = sentence.split()\n",
    "\n",
    "        # Find the adjective-noun\n",
    "        an_indices = [i for i, t in enumerate(tokens) \n",
    "                      if t == w1 and len(tokens) > i+1 and tokens[i+1] == w2]\n",
    "\n",
    "        # Find the target index\n",
    "        if len(an_indices) > 0:\n",
    "            start = an_indices[0]\n",
    "            curr_sentences.append((sentence, an, start))\n",
    "\n",
    "    if len(curr_sentences) > 1:\n",
    "        sentence, an, start = random.choice(curr_sentences)\n",
    "        context_dataset.append((sentence, an, start, relation, label))\n",
    "            \n",
    "print(f'Dataset size: {len(context_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train/validation/test. The split is lexical by adjective, which is often similar to the attribute, to make it more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lexically(dataset, word_index=0):\n",
    "    \"\"\"\n",
    "    Split the dataset to train, test, and validation, such that\n",
    "    the word in word_index (0 = adjective, 1 = noun) doesn't\n",
    "    repeat across sets.\n",
    "    \"\"\"\n",
    "    instances_per_w = defaultdict(list)\n",
    "    [instances_per_w[an.split('_')[word_index]].append(\n",
    "        (sentence, an, start, relation, label)) \n",
    "     for (sentence, an, start, relation, label) in dataset]\n",
    "\n",
    "    train, test, val = [], [], []\n",
    "    train_size = 8 * len(dataset) // 10\n",
    "    val_size = test_size = len(dataset) // 10\n",
    "\n",
    "    words = [w for w, examples in sorted(instances_per_w.items(), key=lambda x: len(x[1]))]\n",
    "    w_index = 0\n",
    "\n",
    "    while len(test) < test_size:\n",
    "        test += instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Test set size: {} (needed: {})'.format(len(test), test_size))\n",
    "\n",
    "    while len(val) < val_size:\n",
    "        val += instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Validation set size: {} (needed: {})'.format(len(val), val_size))\n",
    "\n",
    "    train = [example for i in range(w_index, len(words)) \n",
    "             for example in instances_per_w[words[i]]]\n",
    "    print('Train set size: {} (needed: {})'.format(len(train), train_size))\n",
    "\n",
    "    # Check the label distribution in the test set\n",
    "    ctr = Counter([label for (sentence, an, start, relation, label) in test])\n",
    "    assert(ctr['False'] / ctr['True'] <= 4 and ctr['True'] / ctr['False'] <= 4)\n",
    "    \n",
    "    # Make sure the split is lexical among verbs\n",
    "    test_words = [an.split('_')[word_index] for sentence, an, start, relation, label in test]\n",
    "    train_words = [an.split('_')[word_index] for sentence, an, start, relation, label in train]\n",
    "    val_words = [an.split('_')[word_index] for sentence, an, start, relation, label in val]\n",
    "    assert(len(set(train_words).intersection(set(val_words))) == 0)\n",
    "    assert(len(set(train_words).intersection(set(test_words))) == 0)\n",
    "    assert(len(set(test_words).intersection(set(val_words))) == 0)\n",
    "\n",
    "    print(f'Sizes: train = {len(train)}, test = {len(test)}, validation = {len(val)}')\n",
    "    return train, test, val\n",
    "    \n",
    "\n",
    "data_dir = '../diagnostic_classifiers/data/an_attribute_selection'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "train, test, val = split_lexically(context_dataset, word_index=1)\n",
    "\n",
    "for s, filename in zip([train, test, val], ['train', 'test', 'val']):\n",
    "    with codecs.open(os.path.join(data_dir, f'{filename}.jsonl'), 'w', 'utf-8') as f_out:\n",
    "        for sentence, an, start, relation, label in s:\n",
    "            a, n = an.split('_')\n",
    "            paraphrase = f'{a} refers to the {relation} of {n}'\n",
    "            example_dict = {'sentence' : sentence, 'start': start, 'end': start + 1, \n",
    "                            'label': label, 'paraphrase': paraphrase}\n",
    "            f_out.write(json.dumps(example_dict) + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: majority baseline is not too strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_label_per_word(train_set, word_index=0):\n",
    "    \"\"\"\n",
    "    Compute the majority label by relation\n",
    "    \"\"\"\n",
    "    per_word_labels = defaultdict(list)\n",
    "    for _, an, _, _, label in train_set:\n",
    "        per_word_labels[an.split('_')[word_index]].append(label)\n",
    "    \n",
    "    per_word_majority_label = {w: Counter(curr_labels).most_common(1)[0][0] \n",
    "                               for w, curr_labels in per_word_labels.items()}\n",
    "    return per_word_majority_label   \n",
    "\n",
    "\n",
    "test_labels = [label for _, _, _, _, label in test]\n",
    "overall_majority_label = Counter([label for _, _, _, _, label in train]).most_common(1)[0][0]\n",
    "test_predictions_all = [overall_majority_label] * len(test)\n",
    "print('Majority overall: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_all)))\n",
    "\n",
    "per_adj_majority_label = get_majority_label_per_word(train, 0)\n",
    "test_adj = [an.split('_')[0] for _, an, _, _, _ in test]\n",
    "test_predictions_adj = [per_adj_majority_label.get(a, overall_majority_label) \n",
    "                        for a in test_adj]\n",
    "print('Majority by adjective: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_adj)))\n",
    "\n",
    "per_noun_majority_label = get_majority_label_per_word(train, 1)\n",
    "test_nouns = [an.split('_')[1] for _, an, _, _, _ in test]\n",
    "test_predictions_nouns = [per_noun_majority_label.get(n, overall_majority_label) \n",
    "                          for n in test_nouns]\n",
    "print('Majority by noun: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_nouns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../diagnostic_classifiers/data/an_attribute_selection'\n",
    "\n",
    "sets = []\n",
    "for filename in ['train', 'test', 'val']:\n",
    "    curr = []\n",
    "    with codecs.open(os.path.join(data_dir, f'{filename}.jsonl'), 'r', 'utf-8') as f_in:\n",
    "        curr = []\n",
    "        for line in f_in:\n",
    "            ex = json.loads(line.strip())\n",
    "            tokens = ex['sentence'].split()\n",
    "            an = ' '.join((tokens[ex['start']], tokens[ex['end']]))\n",
    "            curr.append((ex['sentence'], an, ex['relation'], ex['label']))\n",
    "        \n",
    "        sets.append(curr)\n",
    "        \n",
    "train, test, val = sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-annotated the dataset to compute human performance. \n",
    "We assume the annotation results are found under `preprocessing/annotation/an_classification/batch_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_results(result_file, remove_bad_workers=False):\n",
    "    \"\"\"\n",
    "    Load the batch results from the CSV\n",
    "    :param result_file: the batch results CSV file from MTurk\n",
    "    :return: the workers and the answers\n",
    "    \"\"\"\n",
    "    answer_by_worker, answer_by_hit = defaultdict(dict), defaultdict(dict)\n",
    "    workers = set()\n",
    "    incorrect = set()\n",
    "    workers_wrong_answers = defaultdict(int)\n",
    "    hit_id_to_instance = {}\n",
    "    \n",
    "    with codecs.open(result_file, 'r', 'utf-8') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in reader:\n",
    "            hit_id = row['HITId']\n",
    "            worker_id = row['WorkerId']\n",
    "\n",
    "            # Input fields\n",
    "            sent = row['Input.sent']\n",
    "            orig_label = row['Input.orig_label']\n",
    "            attribute = row['Input.attribute']\n",
    "            \n",
    "            tokens = sent.split()\n",
    "            \n",
    "            try:\n",
    "                a = [t for t in tokens if t.startswith('<mark>')][0].replace('<mark>', '')\n",
    "                n = [t for t in tokens if t.endswith('</mark>')][0].replace('</mark>', '')\n",
    "                sent = sent.replace('<mark>', '').replace('</mark>', '').strip()\n",
    "            except:\n",
    "                print(f'Warning: skipped \"{sentence}\"')\n",
    "                continue\n",
    "            \n",
    "            hit_id_to_instance[hit_id] = (sent, a, n, attribute)\n",
    "            \n",
    "            # Answer fields\n",
    "            if row['Answer.label.yes'].lower() == 'true':\n",
    "                answer = 'true'\n",
    "            elif row['Answer.label.no'].lower() == 'true':\n",
    "                answer = 'false'\n",
    "            # Incorrect\n",
    "            else:\n",
    "                incorrect.add(hit_id)\n",
    "                continue\n",
    "\n",
    "            if orig_label.lower() != answer:\n",
    "                workers_wrong_answers[worker_id] += 1\n",
    "                \n",
    "            workers.add(worker_id)\n",
    "            answer_by_worker[worker_id][hit_id] = answer\n",
    "            answer_by_hit[hit_id][worker_id] = answer\n",
    "            \n",
    "    # Remove HITs that were annotated as incorrect by at least one worker\n",
    "    answer_by_hit = {hit_id: answers_by_hit_id \n",
    "                     for hit_id, answers_by_hit_id in answer_by_hit.items()\n",
    "                     if hit_id not in incorrect}\n",
    "    \n",
    "    new_answer_by_worker = {}\n",
    "    for worker_id, curr_answers in answer_by_worker.items():\n",
    "        new_answer_by_worker[worker_id] = {hit_id: answer \n",
    "                                           for hit_id, answer in curr_answers.items()\n",
    "                                           if hit_id not in incorrect}\n",
    "        \n",
    "    answer_by_worker = new_answer_by_worker\n",
    "    num_answers = sum([len(answers_by_worker_id) \n",
    "                       for answers_by_worker_id in answer_by_worker.values()])\n",
    "    \n",
    "    if remove_bad_workers:\n",
    "        workers_wrong_answers = {worker_id: n * 100.0 / len(answer_by_worker[worker_id])\n",
    "                                 for worker_id, n in workers_wrong_answers.items()}\n",
    "\n",
    "        # Remove bad workers: workers that disagreed with many of the previous annotation \n",
    "        bad_workers = {worker_id \n",
    "                       for worker_id, per in workers_wrong_answers.items() if per > 33}\n",
    "        print(f'Removing {len(bad_workers)} bad workers:\\n{bad_workers}')\n",
    "\n",
    "        answer_by_worker = {worker_id: answers_by_worker_id \n",
    "                            for worker_id, answers_by_worker_id in answer_by_worker.items()\n",
    "                            if worker_id not in bad_workers}\n",
    "\n",
    "        for hit_id in answer_by_hit.keys():\n",
    "            answers_by_hit_id = answer_by_hit[hit_id]\n",
    "            answer_by_hit[hit_id] = {worker_id: answer \n",
    "                                      for worker_id, answer in answers_by_hit_id.items()\n",
    "                                      if worker_id not in bad_workers}\n",
    "\n",
    "        num_answers_after_filtering = sum([len(answers_by_worker_id) \n",
    "                                           for answers_by_worker_id in answer_by_worker.values()])\n",
    "        print('Final: {} answers, removed {}.'.format(\n",
    "            num_answers_after_filtering, \n",
    "            num_answers - num_answers_after_filtering))\n",
    "    \n",
    "    return workers, answer_by_worker, answer_by_hit, incorrect, hit_id_to_instance\n",
    "\n",
    "\n",
    "results_file = 'an_classification/batch_results.csv'\n",
    "workers, answer_by_worker, answer_by_hit, incorrect, hit_id_to_instance = load_batch_results(\n",
    "    results_file, remove_bad_workers=True)\n",
    "print(f'Loaded results from {results_file}, loaded {len(answer_by_hit)} answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes Fleiss Kappa and percent of agreement between the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement(answer_by_hit):\n",
    "    \"\"\"\n",
    "    Compute workers' agreement (Fleiss Kappa and percent) \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    percent = 0\n",
    "    \n",
    "    for hit_id, worker_answers in answer_by_hit.items():\n",
    "        curr = [0, 0]\n",
    "\n",
    "        for answer in worker_answers.values():\n",
    "            label = 1 if answer == 'true' else 0\n",
    "            curr[label] += 1\n",
    "            \n",
    "        if sum(curr) == 3:\n",
    "            data.append(curr)\n",
    "            curr_agreement = sum([max(0, a-1) for a in curr])        \n",
    "            percent += curr_agreement\n",
    "\n",
    "    kappa = fleiss_kappa(data)\n",
    "    percent = percent * 100.0 / (len(data) * 2)\n",
    "    return kappa, percent\n",
    "\n",
    "\n",
    "kappa, percent = compute_agreement(answer_by_hit)\n",
    "print('Fleiss Kappa={:.3f}, Percent={:.2f}%'.format(kappa, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the workers majority which we will use to estimate human performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_majority(results):\n",
    "    \"\"\"\n",
    "    Compute the majority label from the worker answers    \n",
    "    :param results: HIT ID to worker answers dictionary\n",
    "    \"\"\"\n",
    "    distribution = { hit_id : Counter(sent_results.values())\n",
    "                    for hit_id, sent_results in results.items() }\n",
    "    \n",
    "    dataset = {}\n",
    "    for hit_id, dist in distribution.items():\n",
    "        if len(dist) > 0 and dist.most_common(1)[0][1] >= 2:\n",
    "            sentence, a, n, relation = hit_id_to_instance[hit_id]\n",
    "            sentence = sentence.lower().replace(' ', '')\n",
    "            label = dist.most_common(1)[0][0]\n",
    "            dataset[(sentence, ' '.join((a, n)), relation)] = label\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "human_annotations = compute_majority(answer_by_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the human performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_compared = [1 if human_annotations[\n",
    "    (sentence.lower().replace(' ', ''), an, relation)] == label.lower() else 0 \n",
    "                  for sentence, an, relation, label in test\n",
    "                  if (sentence.lower().replace(' ', ''), an, relation) in human_annotations]\n",
    "            \n",
    "human_accuracy = sum(items_compared) * 100.0 / len(items_compared)\n",
    "\n",
    "print('Number of examples: {}, accuracy: {:.3f}'.format(len(items_compared), human_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexcomp)",
   "language": "python",
   "name": "lexcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
