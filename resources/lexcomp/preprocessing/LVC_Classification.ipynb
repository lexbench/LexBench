{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light Verb Constructions\n",
    "### Distinguishing LVCs from verb + argument\n",
    "\n",
    "We use the dataset in [Tu and Roth (2011)](https://dl.acm.org/citation.cfm?id=2021131) which contains 2,162 sentences from BNC in which a potential light verb construction was found (with verbs from among the 6 most frequently used English light verbs: _do, get, give, have, make_ and _take_), annotated to whether it is an LVC in this context or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(133)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter, defaultdict\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('lvc/lvc_BNC.txt'):\n",
    "    !mkdir -p lvc\n",
    "    !wget http://cogcomp.org/software/tools/MWE_LVC_token.tar.gz\n",
    "    !tar -zxvf MWE_LVC_token.tar.gz\n",
    "    !mv tokenLVC/lvc_BNC.txt lvc\n",
    "    !rm -r tokenLVC\n",
    "    !rm -r MWE_LVC_token.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('lvc/lvc_BNC.txt', 'r', 'utf-8') as f_in:\n",
    "    dataset = [line.strip().split('\\t') for line in f_in]\n",
    "    \n",
    "# The dataset fields are: bnc_id, span text, and label (+/-).\n",
    "dataset = [(bnc_id, span_text, 'true' if label == '+' else 'false') \n",
    "           for bnc_id, span_text, label in dataset]\n",
    "\n",
    "print('Dataset size: {}'.format(len(dataset)))    \n",
    "print('\\n'.join(['\\t'.join(item) for item in dataset[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset to train/validation/test. The split is lexical by auxiliary verb, to make it more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lexically(dataset, word_index=0):\n",
    "    \"\"\"\n",
    "    Split the dataset to train, test, and validation, such that\n",
    "    the word in index 0 (auxiliary verb) or -1 (noun) doesn't repeat across sets.\n",
    "    \"\"\"\n",
    "    instances_per_w = defaultdict(list)\n",
    "    [instances_per_w[span_text.split('_')[word_index]].append(\n",
    "        (bnc_id, span_text, label)) \n",
    "     for (bnc_id, span_text, label) in dataset]\n",
    "\n",
    "    train, test, val = [], [], []\n",
    "    train_size = 8 * len(dataset) // 10\n",
    "    val_size = test_size = len(dataset) // 10\n",
    "\n",
    "    words = [w for w, examples in sorted(instances_per_w.items(), key=lambda x: len(x[1]))]\n",
    "    w_index = 0\n",
    "\n",
    "    while len(test) < test_size:\n",
    "        test += instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Test set size: {} (needed: {})'.format(len(test), test_size))\n",
    "\n",
    "    while len(val) < val_size:\n",
    "        val += instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Validation set size: {} (needed: {})'.format(len(val), val_size))\n",
    "\n",
    "    train = [example for i in range(w_index, len(words)) \n",
    "             for example in instances_per_w[words[i]]]\n",
    "    print('Train set size: {} (needed: {})'.format(len(train), train_size))\n",
    "\n",
    "    # Check the label distribution in the test set\n",
    "    ctr = Counter([label for (bnc_id, span_text, label) in test])\n",
    "    assert(ctr['false'] / ctr['true'] <= 4 and ctr['true'] / ctr['false'] <= 4)\n",
    "    \n",
    "    # Make sure the split is lexical among verbs\n",
    "    test_words = [span_text.split('_')[word_index] for _, span_text, _ in test]\n",
    "    train_words = [span_text.split('_')[word_index] for _, span_text, _ in train]\n",
    "    val_words = [span_text.split('_')[word_index] for _, span_text, _ in val]\n",
    "    assert(len(set(train_words).intersection(set(val_words))) == 0)\n",
    "    assert(len(set(train_words).intersection(set(test_words))) == 0)\n",
    "    assert(len(set(test_words).intersection(set(val_words))) == 0)\n",
    "\n",
    "    print(f'Sizes: train = {len(train)}, test = {len(test)}, validation = {len(val)}')\n",
    "    return train, test, val\n",
    "    \n",
    "\n",
    "data_dir = '../diagnostic_classifiers/data/lvc_classification'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "train, test, val = split_lexically(dataset)\n",
    "\n",
    "for s, filename in zip([train, test, val], ['train', 'test', 'val']):\n",
    "    with codecs.open(os.path.join(data_dir, 'ids_{}.jsonl'.format(filename)), 'w', 'utf-8') as f_out:\n",
    "        for bnc_id, span_text, label in s:\n",
    "            example = {'bnc_id': bnc_id, 'span_text': span_text.replace('_', ' '), 'label': label}\n",
    "            f_out.write(json.dumps(example) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: majority baseline is not too strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_label_per_word(train_set, word_index=0):\n",
    "    \"\"\"\n",
    "    Compute the majority label by word\n",
    "    :word_index: 0 for verb, -1 for noun\n",
    "    \"\"\"\n",
    "    per_word_labels = defaultdict(list)\n",
    "    for _, span_text, label in train_set:\n",
    "        w = span_text.split('_')[word_index]\n",
    "        per_word_labels[w].append(label)\n",
    "        \n",
    "    per_word_majority_label = {w: Counter(curr_labels).most_common(1)[0][0] \n",
    "                               for w, curr_labels in per_word_labels.items()}\n",
    "    return per_word_majority_label   \n",
    "\n",
    "\n",
    "test_labels = [label for _, _, label in test]\n",
    "overall_majority_label = Counter([label for _, _, label in train]).most_common(1)[0][0]\n",
    "test_predictions_all = [overall_majority_label] * len(test)\n",
    "print('Majority overall: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_all)))\n",
    "\n",
    "per_verb_majority_label = get_majority_label_per_word(train, word_index=0)\n",
    "test_verbs = [span_text.split('_')[0] for _, span_text, _ in test]\n",
    "test_predictions_verb = [per_verb_majority_label.get(v, overall_majority_label) \n",
    "                         for v in test_verbs]\n",
    "print('Majority by verb: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_verb)))\n",
    "\n",
    "per_noun_majority_label = get_majority_label_per_word(train, word_index=-1)\n",
    "test_nouns = [span_text.split('_')[-1] for _, span_text, _ in test]\n",
    "test_predictions_noun = [per_noun_majority_label.get(n, overall_majority_label) \n",
    "                         for n in test_nouns]\n",
    "print('Majority by noun: {:.2f}%'.format(\n",
    "    100.0 * accuracy_score(test_labels, test_predictions_noun)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dataset is given with the sentence IDs and without the sentences themselves, to comply with the BNC corpus license. To get the sentences, follow the instructions in the repository README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-annotated a sample from the test set to compute human performance. \n",
    "\n",
    "First, let's create the batch instances. We show the annotators the candidate LVC (e.g. *make a difficult decision*) in a sentence, and ask them to mark all that applies:\n",
    "\n",
    "1. It describes an action of *make decision*.\n",
    "2. It describes an action of *making something*, in the common meaning of *make*.\n",
    "3. The essence of the action is described by *decision*.\n",
    "4. The span could be rephrased without *make* but with a verb like *decide* without changing the meaning of the sentence.\n",
    "5. The sentence does not make sense or is ungrammatical.\n",
    "\n",
    "We consider something as a LVC if: 1) the answer to 2 is no and the answer to 3 is yes; or 2) the answer to 4 is yes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('../diagnostic_classifiers/data/lvc_classification/test.jsonl', 'r', 'utf-8') as f_in:\n",
    "    field_names = ['sent_id', 'sent', 'aux', 'noun', 'span_text', \n",
    "                   'simple_lvc', 'substitute_verb', 'original_label']\n",
    "\n",
    "    with codecs.open('lvc/batch_instances.csv', 'w', 'utf-8') as f_out:\n",
    "        writer = csv.DictWriter(f_out, fieldnames=field_names)\n",
    "        writer.writeheader()\n",
    "                \n",
    "        for line in f_in:\n",
    "            example = json.loads(line.strip())\n",
    "\n",
    "            # Create the highlighted sentence\n",
    "            start, end = example['start'], example['end']\n",
    "            sent = example['sentence']\n",
    "            sent = sent[0].upper() + sent[1:]\n",
    "            tokens = sent.split()\n",
    "            tokens[start] = '<mark>' + tokens[start]\n",
    "            tokens[end] += '</mark>'\n",
    "            new_sent = ' '.join(tokens)\n",
    "\n",
    "            span_tokens = example['span_text'].split()\n",
    "            aux = span_tokens[0]\n",
    "            noun = span_tokens[-1]\n",
    "            simple_lvc = ' '.join((aux, noun))\n",
    "            \n",
    "            syns = wn.synsets(noun, 'n')\n",
    "\n",
    "            potential_lemmas = { verb.name() \n",
    "                                for syn in syns\n",
    "                                for lemma in syn.lemmas()\n",
    "                                for verb in lemma.derivationally_related_forms()\n",
    "                                if (fuzz.ratio(lemma.name(), noun) >= 50  or \n",
    "                                    fuzz.partial_ratio(lemma.name(), noun) >= 50)\n",
    "                                and verb.synset().pos() == 'v'\n",
    "                               }\n",
    "\n",
    "            if len(potential_lemmas) > 0:\n",
    "                sorted_verbs = list(sorted([(verb, \n",
    "                                             fuzz.ratio(verb, noun) * fuzz.partial_ratio(verb, noun)) \n",
    "                                            for verb in potential_lemmas],\n",
    "                                           key=lambda x: x[1]))\n",
    "                \n",
    "                best_verb = sorted_verbs[-1]\n",
    "                \n",
    "                if best_verb[1] >= 50 or fuzz.partial_ratio(best_verb[0], noun) >= 50:\n",
    "                    new_instance = {'sent_id': example['bnc_id'],\n",
    "                                    'sent': new_sent,\n",
    "                                    'aux': aux,\n",
    "                                    'noun': noun,\n",
    "                                    'span_text': example['span_text'],\n",
    "                                    'original_label': example['label'],\n",
    "                                    'simple_lvc': simple_lvc,\n",
    "                                    'substitute_verb': best_verb[0]\n",
    "                                   }\n",
    "                \n",
    "                    writer.writerow(new_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the annotation results are found under `preprocessing/annotation/lvc/batch_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_results(result_file, remove_bad_workers=False):\n",
    "    \"\"\"\n",
    "    Load the batch results from the CSV\n",
    "    :param result_file: the batch results CSV file from MTurk\n",
    "    :return: the workers and the answers\n",
    "    \"\"\"\n",
    "    answer_by_worker, answer_by_hit = defaultdict(dict), defaultdict(dict)\n",
    "    workers = set()\n",
    "    incorrect = set()\n",
    "    workers_wrong_answers = defaultdict(int)\n",
    "    inputs_by_sent_id = {}\n",
    "    \n",
    "    with codecs.open(result_file, 'r', 'utf-8') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in reader:\n",
    "            hit_id = row['HITId']\n",
    "            worker_id = row['WorkerId']\n",
    "\n",
    "            # Input fields\n",
    "            sent_id = row['Input.sent_id']\n",
    "            sent = row['Input.sent'].replace('<mark>', '').replace('</mark>', '')\n",
    "            aux = row['Input.aux']\n",
    "            noun = row['Input.noun']\n",
    "            span_text = row['Input.span_text']\n",
    "            simple_lvc = row['Input.simple_lvc']\n",
    "            substitute_verb = row['Input.substitute_verb']\n",
    "            original_label = row['Input.original_label']\n",
    "            inputs_by_sent_id[sent_id] = (sent, aux, noun, span_text, simple_lvc, \n",
    "                                          substitute_verb, original_label)\n",
    "            \n",
    "            # Answer fields\n",
    "            is_simple_lvc = row['Answer.answer.simple_lvc'].lower()\n",
    "            meaning_noun = row['Answer.answer.meaning_noun'].lower()\n",
    "            meaning_verb = row['Answer.answer.meaning_verb'].lower()\n",
    "            is_substitutable = row['Answer.answer.substitute'].lower()\n",
    "            is_incorrect = row['Answer.answer.incorrect'].lower()\n",
    "            \n",
    "            # Incorrect\n",
    "            if is_incorrect == 'true':\n",
    "                incorrect.add(sent_id)\n",
    "                continue\n",
    "                \n",
    "            # Compute aggregated label\n",
    "            lvc = (meaning_noun and not meaning_verb) or is_substitutable\n",
    "            answer = (is_simple_lvc, meaning_noun, meaning_verb, is_substitutable, lvc)\n",
    "            \n",
    "            if original_label != lvc:\n",
    "                workers_wrong_answers[worker_id] += 1\n",
    "                \n",
    "            workers.add(worker_id)\n",
    "            answer_by_worker[worker_id][sent_id] = answer\n",
    "            answer_by_hit[sent_id][worker_id] = answer\n",
    "            \n",
    "    # Remove HITs that were annotated as incorrect by at least one worker\n",
    "    answer_by_hit = {sent_id: answers_by_sent_id \n",
    "                     for sent_id, answers_by_sent_id in answer_by_hit.items()\n",
    "                     if sent_id not in incorrect}\n",
    "    \n",
    "    new_answer_by_worker = {}\n",
    "    for worker_id, curr_answers in answer_by_worker.items():\n",
    "        new_answer_by_worker[worker_id] = {sent_id: answer \n",
    "                                           for sent_id, answer in curr_answers.items()\n",
    "                                           if sent_id not in incorrect}\n",
    "        \n",
    "    answer_by_worker = new_answer_by_worker\n",
    "    num_answers = sum([len(answers_by_worker_id) \n",
    "                       for answers_by_worker_id in answer_by_worker.values()])\n",
    "    \n",
    "    if remove_bad_workers:\n",
    "        workers_wrong_answers = {worker_id: n * 100.0 / len(answer_by_worker[worker_id])\n",
    "                                 for worker_id, n in workers_wrong_answers.items()}\n",
    "\n",
    "        # Remove bad workers: workers that disagreed with many of the previous annotation \n",
    "        bad_workers = {worker_id \n",
    "                       for worker_id, per in workers_wrong_answers.items() if per > 32}\n",
    "        print(f'Removing {len(bad_workers)} bad workers:\\n{bad_workers}')\n",
    "\n",
    "        answer_by_worker = {worker_id: answers_by_worker_id \n",
    "                            for worker_id, answers_by_worker_id in answer_by_worker.items()\n",
    "                            if worker_id not in bad_workers}\n",
    "\n",
    "        for sent_id in answer_by_hit.keys():\n",
    "            answers_by_sent_id = answer_by_hit[sent_id]\n",
    "            answer_by_hit[sent_id] = {worker_id: answer \n",
    "                                      for worker_id, answer in answers_by_sent_id.items()\n",
    "                                      if worker_id not in bad_workers}\n",
    "\n",
    "        num_answers_after_filtering = sum([len(answers_by_worker_id) \n",
    "                                           for answers_by_worker_id in answer_by_worker.values()])\n",
    "        print('Final: {} answers, removed {}.'.format(\n",
    "            num_answers_after_filtering, \n",
    "            num_answers - num_answers_after_filtering))\n",
    "    \n",
    "    return workers, answer_by_worker, answer_by_hit, incorrect, inputs_by_sent_id\n",
    "\n",
    "\n",
    "results_file = 'lvc/batch_results.csv'\n",
    "workers, answer_by_worker, answer_by_hit, incorrect, inputs_by_sent_id = load_batch_results(\n",
    "    results_file, remove_bad_workers=True)\n",
    "print(f'Loaded results from {results_file}')\n",
    "print(f'Removed {len(incorrect)} incorrect instances.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes Fleiss Kappa and percent of agreement between the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement(answer_by_hit, field_index):\n",
    "    \"\"\"\n",
    "    Compute workers' agreement (Fleiss Kappa and percent) \n",
    "    :field_index which answer field to compute agreement on\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    percent = 0\n",
    "    \n",
    "    for sent_id, worker_answers in answer_by_hit.items():\n",
    "        curr = [0, 0]\n",
    "\n",
    "        for answer in worker_answers.values():\n",
    "            label = 1 if answer[field_index] == 'true' else 0\n",
    "            curr[label] += 1\n",
    "\n",
    "        if sum(curr) == 3:\n",
    "            data.append(curr)\n",
    "            curr_agreement = sum([max(0, a-1) for a in curr])        \n",
    "            percent += curr_agreement\n",
    "\n",
    "    kappa = fleiss_kappa(data)\n",
    "    percent = percent * 100.0 / (len(data) * 2)\n",
    "    return kappa, percent\n",
    "\n",
    "\n",
    "for field_index, field_name in enumerate([\n",
    "    'is_simple_lvc', 'meaning_noun', 'meaning_verb', 'is_substitutable', 'lvc']):\n",
    "    kappa, percent = compute_agreement(answer_by_hit, field_index)\n",
    "    print('Field: {}, Fleiss Kappa={:.3f}, Percent={:.2f}%'.format(field_name, kappa, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the workers majority which we will use to estimate human performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_majority(results):\n",
    "    \"\"\"\n",
    "    Compute the majority label from the worker answers    \n",
    "    :param results: sentence ID to worker answers dictionary\n",
    "    \"\"\"\n",
    "    distribution = { sent_id : Counter([answer[-1] for answer in sent_results.values()])\n",
    "                    for sent_id, sent_results in results.items() }\n",
    "    \n",
    "    dataset = [{'sent_id': sent_id, \n",
    "                'span_text': inputs_by_sent_id[sent_id][3],\n",
    "                'label': dist.most_common(1)[0][0]}\n",
    "               for sent_id, dist in distribution.items()\n",
    "               if len(dist) > 0 and dist.most_common(1)[0][1] >= 2]\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "human_annotations = compute_majority(answer_by_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the human performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_by_sent_id = {e['sent_id']: e['label'] for e in human_annotations}\n",
    "test_annotations = {sent_id: label for sent_id, _, label in test}\n",
    "\n",
    "human_accuracy = sum([1 if label == test_annotations[sent_id] else 0 \n",
    "                      for sent_id, label in gold_by_sent_id.items()\n",
    "                     ]) * 100.0 / len(gold_by_sent_id)\n",
    "\n",
    "print('Number of examples: {}, accuracy: {:2.2f}'.format(\n",
    "    len(gold_by_sent_id), human_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexcomp)",
   "language": "python",
   "name": "lexcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
