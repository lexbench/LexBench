{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Compounds \n",
    "### Paraphrasing\n",
    "\n",
    "We use the dataset from [SemEval 2013 Task 4: Free Paraphrases of Noun Compounds](https://www.cs.york.ac.uk/semeval-2013/task4/index.php). In the original task, given a two-word noun compound, the participating system was asked to produce an explicitly ranked list of its free-form paraphrases. The list was automatically compared and evaluated against a similarly ranked list of paraphrases proposed by human annotators. We cast this as a classification problem whose input is a noun compound and a paraphrase and whose output is a binary value indicating whether the paraphrase is a correct explication of the compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(133)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import spacy\n",
    "import shutil\n",
    "import codecs\n",
    "import random\n",
    "import fileinput\n",
    "\n",
    "from nltk import agreement\n",
    "from itertools import count\n",
    "from collections import Counter, defaultdict\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('nc_relations'):\n",
    "    !mkdir -p nc_relations\n",
    "    !wget https://raw.githubusercontent.com/vered1986/panic/master/paraphrasing/data/semeval_2013/test_gold.txt\n",
    "    !wget https://raw.githubusercontent.com/vered1986/panic/master/paraphrasing/data/semeval_2013/train_gold.txt\n",
    "    !mv test_gold.txt nc_relations\n",
    "    !mv train_gold.txt nc_relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep maximum of 3 paraphrases for each noun compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_paraphrasing_dataset(file_name):\n",
    "    \"\"\"\n",
    "    Load the SemEval 2013 paraphrase task dataset\n",
    "    return a list of tuples of noun compound and paraphrase\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    verbs_by_nc = defaultdict(list)\n",
    "    \n",
    "    with codecs.open(file_name, 'r', 'utf-8') as f_in:\n",
    "        for line in f_in:\n",
    "            keep = False\n",
    "            data = line.strip().split('\\t')\n",
    "            \n",
    "            if len(data) != 4:\n",
    "                continue\n",
    "                \n",
    "            w1, w2, paraphrase, score = data\n",
    "            nc = '_'.join((w1, w2))\n",
    "            \n",
    "            # Only keep paraphrases with verbs\n",
    "            tokens = nlp(paraphrase)\n",
    "            if len(tokens) < 4:\n",
    "                continue \n",
    "                \n",
    "            # Require a specific verb\n",
    "            for t in tokens:\n",
    "                if t.pos_ == 'VERB' and t.lemma_ not in {\n",
    "                    'involve', 'concern', 'regard', 'discuss', \n",
    "                    'happen', 'deal', 'relate', 'refer'}:\n",
    "                    keep = True\n",
    "                    verbs_by_nc[nc].append(t.lemma_)\n",
    "                \n",
    "            if keep:\n",
    "                dataset.append((nc, paraphrase))\n",
    "            \n",
    "    return dataset, verbs_by_nc\n",
    "            \n",
    "    \n",
    "curr_dir = 'nc_relations'\n",
    "nlp = spacy.load('en')\n",
    "train_paraphrases, verbs_by_nc = load_paraphrasing_dataset(\n",
    "    os.path.join(curr_dir, 'train_gold.txt'))\n",
    "test_paraphrases, verbs_by_nc2 = load_paraphrasing_dataset(\n",
    "    os.path.join(curr_dir, 'test_gold.txt'))\n",
    "dataset = train_paraphrases + test_paraphrases\n",
    "verbs_by_nc = {nc: verbs + verbs_by_nc2.get(nc, []) for nc, verbs in verbs_by_nc.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add sentences from Wikipedia - find sentences for each noun compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the noun compounds in the dataset\n",
    "all_ncs = set([nc for nc, _ in dataset])\n",
    "print('Number of noun compounds: {}'.format(len(all_ncs)))\n",
    "\n",
    "corpus = os.path.expanduser('~/corpora/text/en_corpus_tokenized') # change to your corpus path\n",
    "out_dir = 'nc_relations/sentences'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "    \n",
    "NUM_PARALLEL = 60\n",
    "\n",
    "with codecs.open(os.path.join('nc_relations', 'commands.sh'), 'w', 'utf-8') as f_out:\n",
    "    f_out.write('#!/bin/bash\\n')\n",
    "    commands_written = 0\n",
    "    for nc in all_ncs:\n",
    "        nc = nc.replace(\"'\", '')\n",
    "        \n",
    "        # Already have sentences from the previous dataset\n",
    "        if os.path.exists('nc_classification/sentences/{}'.format(nc)):\n",
    "            shutil.copyfile('nc_classification/sentences/{}'.format(nc), \n",
    "                            'nc_relations/sentences/{}'.format(nc))\n",
    "        else:\n",
    "            f_out.write('grep -i \"{}\" {} > sentences/{} &\\n'.format(nc.replace('_', ' '), corpus, nc))\n",
    "            commands_written += 1\n",
    "            \n",
    "            if commands_written > 0 and commands_written % NUM_PARALLEL == 0:\n",
    "                f_out.write('wait\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_SENT_LEN = 15\n",
    "MAX_SENT_LEN = 25\n",
    "\n",
    "\n",
    "nc_sentences_filtered = {}\n",
    "for nc in all_ncs:\n",
    "    try:\n",
    "        with codecs.open(os.path.join(out_dir, nc), 'r', 'utf-8') as f_in:\n",
    "            nc_sentences = [line.strip() for line in f_in]\n",
    "\n",
    "        nc_sentences_filtered[nc] = [s for s in nc_sentences \n",
    "                                     if len(s.split()) <= MAX_SENT_LEN and \n",
    "                                     len(s.split()) >= MIN_SENT_LEN]\n",
    "\n",
    "        with codecs.open(os.path.join(out_dir, nc), 'w', 'utf-8') as f_out:\n",
    "            for s in nc_sentences_filtered[nc]:\n",
    "                f_out.write(s + '\\n')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add negative examples. For each noun compound we select as many negative examples as the positive ones. To make the negative examples more difficult, we will take them from paraphrases that appeared with noun compounds with either the same head or the same modifier, and we will make sure that the negative paraphrases don't share any verb lemmas with the positive ones. We keep maximum of 5 positive and 5 negative paraphrases for each noun compound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_paraphrases = defaultdict(list)\n",
    "for nc, paraphrase in dataset:\n",
    "    positive_paraphrases[nc].append(paraphrase)\n",
    "    \n",
    "dataset_with_neg = []\n",
    "\n",
    "for nc in all_ncs:\n",
    "    w1, w2 = nc.split('_')\n",
    "    curr_positive_paraphrases = set([par.replace('a ', '').replace('an ', '') \n",
    "                                     for par in positive_paraphrases[nc]])\n",
    "    \n",
    "    ncs_with_shared_word = [other for other in all_ncs\n",
    "                            if other != nc and (w1 == other.split('_')[0]\n",
    "                                or w2 == other.split('_')[1])\n",
    "                           ]\n",
    "\n",
    "    if len(ncs_with_shared_word) == 0:\n",
    "        continue\n",
    "        \n",
    "    curr_verbs = set(verbs_by_nc.get(nc, []))\n",
    "    negative_paraphrases = set([par.replace(nc2.split('_')[0], w1).replace(\n",
    "        nc2.split('_')[1], w2).replace('a ', '').replace('an ', '')\n",
    "                                for nc2 in ncs_with_shared_word\n",
    "                                for par in positive_paraphrases[nc2]\n",
    "                                if len(set([t.lemma_ for t in nlp(par)]).intersection(curr_verbs)) == 0])\n",
    "\n",
    "    negative_paraphrases = [paraphrase for paraphrase in negative_paraphrases \n",
    "                            if w1 in paraphrase and w2 in paraphrase]\n",
    "    \n",
    "    # Same number of negative paraphrases\n",
    "    if len(negative_paraphrases) == 0 or len(positive_paraphrases) == 0:\n",
    "        continue\n",
    "        \n",
    "    if len(curr_positive_paraphrases) > 5:\n",
    "        curr_positive_paraphrases = random.sample(curr_positive_paraphrases, 5)\n",
    "    \n",
    "        \n",
    "    if len(negative_paraphrases) > len(curr_positive_paraphrases):\n",
    "        negative_paraphrases = random.sample(\n",
    "            negative_paraphrases, len(curr_positive_paraphrases))\n",
    "\n",
    "    elif len(curr_positive_paraphrases) > len(negative_paraphrases):\n",
    "        curr_positive_paraphrases = random.sample(\n",
    "            list(curr_positive_paraphrases), len(negative_paraphrases))\n",
    "\n",
    "    print(nc)\n",
    "    print('Positive:\\n=========')\n",
    "    print('\\n'.join(curr_positive_paraphrases) + '\\n')\n",
    "    print('Negative:\\n=========')\n",
    "    print('\\n'.join(negative_paraphrases) + '\\n')\n",
    "    \n",
    "    dataset_with_neg += [(nc, paraphrase, 'True') for paraphrase in curr_positive_paraphrases] \n",
    "    dataset_with_neg += [(nc, paraphrase, 'False') for paraphrase in negative_paraphrases]\n",
    "    \n",
    "    \n",
    "print(f'Dataset size: {len(dataset_with_neg)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the examples to train, test, and validation - lexically by both nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lexically(dataset, word_index=0):\n",
    "    \"\"\"\n",
    "    Split the dataset to train, test, and validation, such that\n",
    "    the word in word_index (0 = modifier, 1 = head) doesn't\n",
    "    repeat across sets.\n",
    "    \"\"\"\n",
    "    instances_per_w = defaultdict(list)\n",
    "    [instances_per_w[nc.split('_')[word_index]].append((nc, paraphrase, label)) \n",
    "     for (nc, paraphrase, label) in dataset]\n",
    "\n",
    "    train, test, val = [], [], []\n",
    "    train_size = 8 * len(dataset) // 10\n",
    "    val_size = test_size = len(dataset) // 10\n",
    "\n",
    "    words = [w for w, examples in sorted(instances_per_w.items(), key=lambda x: len(x[1]))]\n",
    "    w_index = 0\n",
    "\n",
    "    while len(test) < test_size:\n",
    "        test += instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Test set size: {} (needed: {})'.format(len(test), test_size))\n",
    "\n",
    "    while len(val) < val_size:\n",
    "        val += instances_per_w[words[w_index]]\n",
    "        w_index += 1\n",
    "\n",
    "    print('Validation set size: {} (needed: {})'.format(len(val), val_size))\n",
    "\n",
    "    train = [example for i in range(w_index, len(words)) \n",
    "             for example in instances_per_w[words[i]]]\n",
    "    print('Train set size: {} (needed: {})'.format(len(train), train_size))\n",
    "\n",
    "    # Check the label distribution in the test set\n",
    "    ctr = Counter([label for (nc, paraphrase, label) in test])\n",
    "    assert(ctr['False'] / ctr['True'] <= 4 and ctr['True'] / ctr['False'] <= 4)\n",
    "    \n",
    "    # Make sure the split is lexical among verbs\n",
    "    test_words = [nc.split('_')[word_index] for nc, paraphrase, label in test]\n",
    "    train_words = [nc.split('_')[word_index] for nc, paraphrase, label in train]\n",
    "    val_words = [nc.split('_')[word_index] for nc, paraphrase, label in val]\n",
    "    assert(len(set(train_words).intersection(set(val_words))) == 0)\n",
    "    assert(len(set(train_words).intersection(set(test_words))) == 0)\n",
    "    assert(len(set(test_words).intersection(set(val_words))) == 0)\n",
    "\n",
    "    print(f'Sizes: train = {len(train)}, test = {len(test)}, validation = {len(val)}')\n",
    "    return train, test, val\n",
    "    \n",
    "\n",
    "data_dir = '../diagnostic_classifiers/data/nc_relations'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "train, test, val = split_lexically(dataset_with_neg, word_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select a sentence for each example and write it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(set(train).intersection(set(val))) == 0)\n",
    "assert(len(set(train).intersection(set(test))) == 0)\n",
    "assert(len(set(test).intersection(set(val))) == 0)\n",
    "\n",
    "data_dir = '../diagnostic_classifiers/data/nc_relations'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "no_sentences = set()\n",
    "instances = {'train': 0, 'test': 0, 'val': 0}\n",
    "\n",
    "for s, filename in zip([train, test, val], ['train', 'test', 'val']):\n",
    "\n",
    "    # Assert label distribution is relatively balanced\n",
    "    ctr = Counter([item[-1] for item in s])\n",
    "    sorted_ctr = ctr.most_common()\n",
    "    most_common, least_common = sorted_ctr[0], sorted_ctr[-1]\n",
    "    assert(most_common[1] // least_common[1] < 2)\n",
    "    \n",
    "    with codecs.open(os.path.join(data_dir, '{}.jsonl'.format(filename)), 'w', 'utf-8') as f_out:\n",
    "        for nc, paraphrase, label in s:\n",
    "            if nc in no_sentences:\n",
    "                continue\n",
    "            \n",
    "            curr_sentences = nc_sentences_filtered.get(nc, [])\n",
    "            if len(curr_sentences) == 0:\n",
    "                print('No sentences found for {}'.format(nc))\n",
    "                no_sentences.add(nc)\n",
    "                continue\n",
    "            \n",
    "            start = []\n",
    "            while len(start) == 0:\n",
    "                sentence = random.choice(curr_sentences).replace(' +', ' ')\n",
    "                tokens = sentence.lower().split()\n",
    "                start = [i for i, (w1, w2) in enumerate(zip(tokens, tokens[1:])) if nc in '_'.join((w1, w2))]\n",
    "\n",
    "            start = start[0]\n",
    "            example_dict = {'sentence' : sentence, 'start': start, 'end': start + 1, \n",
    "                            'span': nc.replace('_', ' '), 'paraphrase': paraphrase, 'label': label}\n",
    "            f_out.write(json.dumps(example_dict) + '\\n')\n",
    "            instances[filename] += 1\n",
    "            \n",
    "print(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-annotated the dataset to compute human performance. \n",
    "We assume the annotation results are found under `preprocessing/annotation/nc_relations/batch_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_results(result_file, remove_bad_workers=False):\n",
    "    \"\"\"\n",
    "    Load the batch results from the CSV\n",
    "    :param result_file: the batch results CSV file from MTurk\n",
    "    :return: the workers and the answers\n",
    "    \"\"\"\n",
    "    answer_by_worker, answer_by_hit = defaultdict(dict), defaultdict(dict)\n",
    "    workers = set()\n",
    "    incorrect = set()\n",
    "    workers_wrong_answers = defaultdict(int)\n",
    "    hit_id_to_orig_label = {}\n",
    "    \n",
    "    with codecs.open(result_file, 'r', 'utf-8') as f_in:\n",
    "        reader = csv.DictReader(f_in)\n",
    "        for row in reader:\n",
    "            hit_id = row['HITId']\n",
    "            worker_id = row['WorkerId']\n",
    "\n",
    "            # Input fields\n",
    "            sent = row['Input.sent']\n",
    "            orig_label = row['Input.orig_label']\n",
    "            paraphrase = row['Input.paraphrase']\n",
    "            \n",
    "            tokens = sent.split()\n",
    "            \n",
    "            try:\n",
    "                w1 = [t for t in tokens if t.startswith('<mark>')][0].replace('<mark>', '')\n",
    "                w2 = [t for t in tokens if t.endswith('</mark>')][0].replace('</mark>', '')\n",
    "                sent = sent.replace('<mark>', '').replace('</mark>', '').strip()\n",
    "            except:\n",
    "                print(f'Warning: skipped \"{sentence}\"')\n",
    "                continue\n",
    "            \n",
    "            hit_id_to_orig_label[hit_id] = orig_label\n",
    "            \n",
    "            # Answer fields\n",
    "            if row['Answer.label.yes'].lower() == 'true':\n",
    "                answer = 'true'\n",
    "            elif row['Answer.label.no'].lower() == 'true':\n",
    "                answer = 'false'\n",
    "            # Incorrect\n",
    "            else:\n",
    "                incorrect.add(hit_id)\n",
    "                continue\n",
    "\n",
    "            if orig_label.lower() != answer:\n",
    "                workers_wrong_answers[worker_id] += 1\n",
    "                \n",
    "            workers.add(worker_id)\n",
    "            answer_by_worker[worker_id][hit_id] = answer\n",
    "            answer_by_hit[hit_id][worker_id] = answer\n",
    "            \n",
    "    # Remove HITs that were annotated as incorrect by at least one worker\n",
    "    answer_by_hit = {hit_id: answers_by_hit_id \n",
    "                     for hit_id, answers_by_hit_id in answer_by_hit.items()\n",
    "                     if hit_id not in incorrect}\n",
    "    \n",
    "    new_answer_by_worker = {}\n",
    "    for worker_id, curr_answers in answer_by_worker.items():\n",
    "        new_answer_by_worker[worker_id] = {hit_id: answer \n",
    "                                           for hit_id, answer in curr_answers.items()\n",
    "                                           if hit_id not in incorrect}\n",
    "        \n",
    "    answer_by_worker = new_answer_by_worker\n",
    "    num_answers = sum([len(answers_by_worker_id) \n",
    "                       for answers_by_worker_id in answer_by_worker.values()])\n",
    "    \n",
    "    if remove_bad_workers:\n",
    "        workers_wrong_answers = {worker_id: n * 100.0 / len(answer_by_worker[worker_id])\n",
    "                                 for worker_id, n in workers_wrong_answers.items()}\n",
    "\n",
    "        # Remove bad workers: workers that disagreed with many of the previous annotation \n",
    "        bad_workers = {worker_id \n",
    "                       for worker_id, per in workers_wrong_answers.items() if per > 33}\n",
    "        print(f'Removing {len(bad_workers)} bad workers:\\n{bad_workers}')\n",
    "\n",
    "        answer_by_worker = {worker_id: answers_by_worker_id \n",
    "                            for worker_id, answers_by_worker_id in answer_by_worker.items()\n",
    "                            if worker_id not in bad_workers}\n",
    "\n",
    "        for hit_id in answer_by_hit.keys():\n",
    "            answers_by_hit_id = answer_by_hit[hit_id]\n",
    "            answer_by_hit[hit_id] = {worker_id: answer \n",
    "                                      for worker_id, answer in answers_by_hit_id.items()\n",
    "                                      if worker_id not in bad_workers}\n",
    "\n",
    "        num_answers_after_filtering = sum([len(answers_by_worker_id) \n",
    "                                           for answers_by_worker_id in answer_by_worker.values()])\n",
    "        print('Final: {} answers, removed {}.'.format(\n",
    "            num_answers_after_filtering, \n",
    "            num_answers - num_answers_after_filtering))\n",
    "    \n",
    "    return workers, answer_by_worker, answer_by_hit, incorrect, hit_id_to_orig_label\n",
    "\n",
    "\n",
    "results_file = 'nc_relations/batch_results.csv'\n",
    "workers, answer_by_worker, answer_by_hit, incorrect, hit_id_to_orig_label = load_batch_results(\n",
    "    results_file, remove_bad_workers=True)\n",
    "print(f'Loaded results from {results_file}, loaded {len(answer_by_hit)} HITs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes Fleiss Kappa and percent of agreement between the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement(answer_by_hit):\n",
    "    \"\"\"\n",
    "    Compute workers' agreement (Fleiss Kappa and percent) \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    percent = 0\n",
    "    \n",
    "    for hit_id, worker_answers in answer_by_hit.items():\n",
    "        curr = [0, 0]\n",
    "\n",
    "        for answer in worker_answers.values():\n",
    "            label = 1 if answer == 'true' else 0\n",
    "            curr[label] += 1\n",
    "            \n",
    "        if sum(curr) == 3:\n",
    "            data.append(curr)\n",
    "            curr_agreement = sum([max(0, a-1) for a in curr])        \n",
    "            percent += curr_agreement\n",
    "\n",
    "    kappa = fleiss_kappa(data)\n",
    "    percent = percent * 100.0 / (len(data) * 2)\n",
    "    return kappa, percent\n",
    "\n",
    "\n",
    "kappa, percent = compute_agreement(answer_by_hit)\n",
    "print('Fleiss Kappa={:.3f}, Percent={:.2f}%'.format(kappa, percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the workers majority which we will use to estimate human performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_majority(results):\n",
    "    \"\"\"\n",
    "    Compute the majority label from the worker answers    \n",
    "    :param results: HIT ID to worker answers dictionary\n",
    "    \"\"\"\n",
    "    distribution = { hit_id : Counter(sent_results.values())\n",
    "                    for hit_id, sent_results in results.items() }\n",
    "    \n",
    "    dataset = {}\n",
    "    for hit_id, dist in distribution.items():\n",
    "        if len(dist) > 0 and dist.most_common(1)[0][1] >= 2:\n",
    "            label = dist.most_common(1)[0][0]\n",
    "            dataset[hit_id] = label\n",
    "   \n",
    "    return dataset\n",
    "\n",
    "human_annotations = compute_majority(answer_by_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the human performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_compared = [1 if human_annotations[hit_id].lower() == hit_id_to_orig_label[hit_id].lower() else 0 \n",
    "                  for hit_id in human_annotations.keys()]\n",
    "            \n",
    "human_accuracy = sum(items_compared) * 100.0 / len(items_compared)\n",
    "\n",
    "print('Number of examples: {}, accuracy: {:.3f}'.format(len(items_compared), human_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexcomp)",
   "language": "python",
   "name": "lexcomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
